{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b001cbcc97cb66",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CCS-ZCU/EuPaC_shared/blob/master/NOSCEMUS_getting-started.ipynb)\n",
    "\n",
    "This Jupyter notebook has been prepared for the EuPaC Hackathon and provides an easy way to start working with the NOSCEMUS dataset — no need to clone the entire repository or download additional data. It is fully compatible with cloud platforms like Google Colaboratory (click the badge above) and runs without requiring any specialized library installations.\n",
    "\n",
    "As such, it is intended as a starting point for EuPaC participants, including those with minimal coding experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bafbaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in ./.venv/lib/python3.12/site-packages (0.19.6)\n",
      "Requirement already satisfied: geopandas in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: branca>=0.6.0 in ./.venv/lib/python3.12/site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in ./.venv/lib/python3.12/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in ./.venv/lib/python3.12/site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in ./.venv/lib/python3.12/site-packages (from geopandas) (0.11.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in ./.venv/lib/python3.12/site-packages (from geopandas) (2.2.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in ./.venv/lib/python3.12/site-packages (from geopandas) (3.7.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in ./.venv/lib/python3.12/site-packages (from geopandas) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->folium) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Phase 0A: Setup - Install Libraries\n",
    "%pip install folium geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361b2bdaa3269606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:17:07.109340Z",
     "start_time": "2025-05-01T14:17:07.105530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Phase 0B: Setup - Install Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741c876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Full title</th>\n",
       "      <th>In</th>\n",
       "      <th>Year</th>\n",
       "      <th>Place</th>\n",
       "      <th>Publisher/Printer</th>\n",
       "      <th>Era</th>\n",
       "      <th>Form/Genre</th>\n",
       "      <th>Discipline/Content</th>\n",
       "      <th>Original</th>\n",
       "      <th>...</th>\n",
       "      <th>Of interest to</th>\n",
       "      <th>Transkribus text available</th>\n",
       "      <th>Written by</th>\n",
       "      <th>Library and Signature</th>\n",
       "      <th>ids</th>\n",
       "      <th>id</th>\n",
       "      <th>date_min</th>\n",
       "      <th>date_max</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Achrelius, Daniel</td>\n",
       "      <td>Scientiarum magnes recitatus publice anno 1690...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1690</td>\n",
       "      <td>[Turku]</td>\n",
       "      <td>Wall</td>\n",
       "      <td>17th century</td>\n",
       "      <td>Oration</td>\n",
       "      <td>Mathematics, Astronomy/Astrology/Cosmography, ...</td>\n",
       "      <td>Scientiarum magnes(Google Books)</td>\n",
       "      <td>...</td>\n",
       "      <td>MK, JL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>IT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[705665]</td>\n",
       "      <td>705665</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>Achrelius,_Daniel_-_Scientiarum_magnes__Turku_...</td>\n",
       "      <td>1690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acidalius, Valens</td>\n",
       "      <td>Ad Iordanum Brunum Nolanum, Italum</td>\n",
       "      <td>Poematum Iani Lernutii, Iani Gulielmi, Valenti...</td>\n",
       "      <td>1603</td>\n",
       "      <td>Liegnitz, Wrocław</td>\n",
       "      <td>Albert, David</td>\n",
       "      <td>17th century</td>\n",
       "      <td>Panegyric poem</td>\n",
       "      <td>Astronomy/Astrology/Cosmography</td>\n",
       "      <td>Ad Iordanum Brunum (1603)(CAMENA)Ad Iordanum B...</td>\n",
       "      <td>...</td>\n",
       "      <td>MK, IT</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[801745]</td>\n",
       "      <td>801745</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>Janus_Lernutius_et_al__-_Poemata__Liegnitz_160...</td>\n",
       "      <td>1603.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                         Full title  \\\n",
       "0  Achrelius, Daniel  Scientiarum magnes recitatus publice anno 1690...   \n",
       "1  Acidalius, Valens                 Ad Iordanum Brunum Nolanum, Italum   \n",
       "\n",
       "                                                  In  Year              Place  \\\n",
       "0                                                NaN  1690            [Turku]   \n",
       "1  Poematum Iani Lernutii, Iani Gulielmi, Valenti...  1603  Liegnitz, Wrocław   \n",
       "\n",
       "  Publisher/Printer           Era      Form/Genre  \\\n",
       "0              Wall  17th century         Oration   \n",
       "1     Albert, David  17th century  Panegyric poem   \n",
       "\n",
       "                                  Discipline/Content  \\\n",
       "0  Mathematics, Astronomy/Astrology/Cosmography, ...   \n",
       "1                    Astronomy/Astrology/Cosmography   \n",
       "\n",
       "                                            Original  ... Of interest to  \\\n",
       "0                   Scientiarum magnes(Google Books)  ...         MK, JL   \n",
       "1  Ad Iordanum Brunum (1603)(CAMENA)Ad Iordanum B...  ...         MK, IT   \n",
       "\n",
       "  Transkribus text available Written by Library and Signature       ids  \\\n",
       "0                        Yes         IT                   NaN  [705665]   \n",
       "1                        Yes         MK                   NaN  [801745]   \n",
       "\n",
       "       id date_min date_max  \\\n",
       "0  705665   1690.0   1690.0   \n",
       "1  801745   1603.0   1603.0   \n",
       "\n",
       "                                            filename file_year  \n",
       "0  Achrelius,_Daniel_-_Scientiarum_magnes__Turku_...    1690.0  \n",
       "1  Janus_Lernutius_et_al__-_Poemata__Liegnitz_160...    1603.0  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phase 0A: Data Exploration\n",
    "# Display 2 sample DataFrame rows\n",
    "noscemus_metadata = pd.read_csv(\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\")\n",
    "noscemus_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cline_inserted_cell_cols",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in noscemus_metadata:\n",
      "['Author', 'Full title', 'In', 'Year', 'Place', 'Publisher/Printer', 'Era', 'Form/Genre', 'Discipline/Content', 'Original', 'Digital sourcebook', 'Description', 'References', 'Cited in', 'How to cite this entry', 'Internal notes', 'Of interest to', 'Transkribus text available', 'Written by', 'Library and Signature', 'ids', 'id', 'date_min', 'date_max', 'filename', 'file_year']\n"
     ]
    }
   ],
   "source": [
    "# Phase 0B: Data Exploration\n",
    "# Display DataFrame Columns\n",
    "\n",
    "print(\"\\nColumns in noscemus_metadata:\")\n",
    "print(noscemus_metadata.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cline_inserted_cell_inspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'Place':\n",
      "Place\n",
      "Paris                          69\n",
      "Amsterdam                      49\n",
      "Basel                          48\n",
      "Venice                         48\n",
      "London                         40\n",
      "Leipzig                        36\n",
      "Rome                           34\n",
      "Zurich                         33\n",
      "Leiden                         29\n",
      "Frankfurt am Main              26\n",
      "Göttingen                      25\n",
      "Tübingen                       25\n",
      "Nuremberg                      21\n",
      "Bologna                        21\n",
      "Strasbourg                     20\n",
      "Lyon                           19\n",
      "Wittenberg                     17\n",
      "Innsbruck                      16\n",
      "Cologne                        13\n",
      "Padua                          13\n",
      "Naples                         12\n",
      "Florence                       12\n",
      "Leiden, Stockholm, Erlangen    10\n",
      "Halle                          10\n",
      "Antwerp                        10\n",
      "Oxford                          8\n",
      "Copenhagen                      8\n",
      "Vienna                          8\n",
      "Bern                            7\n",
      "Augsburg                        7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of unique values in 'Place': 173\n",
      "Number of missing values in 'Place': 4\n",
      "\n",
      "Sample raw entries (up to first 20 non-null):\n",
      "['[Turku]', 'Liegnitz, Wrocław', 'Salamanca', 'Heidelberg', 'London', 'Oxford', 'Lund', 'Strasbourg', 'Basel', 'Basel', 'Basel', 'Basel', 'Basel', 'Bologna', 'Leipzig', 'Zurich', 'Venice', 'Rome', 'Herborn', 'Frankfurt am Main']\n"
     ]
    }
   ],
   "source": [
    "# Phase 0C: Data Exploration\n",
    "# Inspect Potential Columns\n",
    "# Replace 'candidate_column_name' with a column name from the list above\n",
    "candidate_column_name = 'Place' # <-- CHANGE THIS VALUE \n",
    "\n",
    "if candidate_column_name in noscemus_metadata.columns:\n",
    "    print(f\"\\nUnique values in '{candidate_column_name}':\")\n",
    "    # Display a sample of unique values and their counts\n",
    "    print(noscemus_metadata[candidate_column_name].value_counts().head(30))\n",
    "    print(f\"\\nNumber of unique values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].nunique()}\")\n",
    "    print(f\"Number of missing values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].isnull().sum()}\")\n",
    "    # Show some raw examples of the data in this column\n",
    "    print(\"\\nSample raw entries (up to first 20 non-null):\")\n",
    "    print(noscemus_metadata[candidate_column_name].dropna().head(20).tolist())\n",
    "else:\n",
    "    print(f\"Column '{candidate_column_name}' not found in DataFrame. Please choose from the list printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cline_extract_place_column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174 unique raw place mentions from 'Place'.\n",
      "Sample of raw places (first 50):\n",
      "['[Turku]' 'Liegnitz, Wrocław' 'Salamanca' 'Heidelberg' 'London' 'Oxford'\n",
      " 'Lund' 'Strasbourg' 'Basel' 'Bologna' 'Leipzig' 'Zurich' 'Venice' 'Rome'\n",
      " 'Herborn' 'Frankfurt am Main' 'Turin' 'Florence' 'Alcalá de Henares'\n",
      " 'Leiden' 'Innsbruck' 'London, Westminster Abbey' 'Paris' 'Cambridge'\n",
      " '[Landshut]' '[Ingolstadt]' 'Milan' 'Bergamo' 'Stuttgart' 'Perugia'\n",
      " 'Lyon' 's.l.' 'Amsterdam' '[Wittenberg]' 'Copenhagen' 'Padua' '[Padua]'\n",
      " 'Rimini' 'Büdingen' 'Königsberg' 'Uppsala' 'Stockholm, Uppsala, Turku'\n",
      " 'Leipzig, Desau' 'Würzburg' 'Saint Petersburg' 'Antwerp' 'Graz' 'Aachen'\n",
      " 'Göttingen' 'Târgu Mureș']\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Data Extraction - Extract 'Place' column\n",
    "actual_publication_place_column = 'Place'\n",
    "places_series = noscemus_metadata[actual_publication_place_column].astype(str).str.strip()\n",
    "unique_raw_places = places_series.unique()\n",
    "print(f\"Found {len(unique_raw_places)} unique raw place mentions from '{actual_publication_place_column}'.\")\n",
    "print(\"Sample of raw places (first 50):\")\n",
    "print(unique_raw_places[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cline_geocode_raw_places",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache file found (raw_geocoded_places_cache.csv). Geocoding raw places...\n",
      "Geocoding 174 unique raw place names...\n",
      "Processed 20/174 places...\n",
      "Processed 40/174 places...\n",
      "Processed 60/174 places...\n",
      "Processed 80/174 places...\n",
      "Processed 100/174 places...\n",
      "Processed 120/174 places...\n",
      "Processed 140/174 places...\n",
      "Processed 160/174 places...\n",
      "Saved raw geocoded data to cache: raw_geocoded_places_cache.csv\n",
      "\n",
      "Successfully geocoded 151 places out of 174 unique raw names processed.\n",
      "\n",
      "Sample of geocoded data (first 20 rows):\n",
      "            raw_place       geoname_name  latitude  longitude          country\n",
      "0             [Turku]              Turku  60.45148   22.26869          Finland\n",
      "1   Liegnitz, Wrocław               None       NaN        NaN             None\n",
      "2           Salamanca          Salamanca  40.96882   -5.66388            Spain\n",
      "3          Heidelberg         Heidelberg  49.40768    8.69079          Germany\n",
      "4              London             London  51.50853   -0.12574   United Kingdom\n",
      "5              Oxford             Oxford  39.50700  -84.74523    United States\n",
      "6                Lund               Lund  55.70584   13.19321           Sweden\n",
      "7          Strasbourg         Strasbourg  48.58392    7.74553           France\n",
      "8               Basel              Basel  47.55839    7.57327      Switzerland\n",
      "9             Bologna            Bologna  44.49381   11.33875            Italy\n",
      "10            Leipzig            Leipzig  51.33962   12.37129          Germany\n",
      "11             Zurich             Zurich  47.36667    8.55000      Switzerland\n",
      "12             Venice             Venice  45.43713   12.33265            Italy\n",
      "13               Rome               Rome  41.89193   12.51133            Italy\n",
      "14            Herborn            Herborn  49.74167    6.42778       Luxembourg\n",
      "15  Frankfurt am Main  Frankfurt am Main  50.11552    8.68417          Germany\n",
      "16              Turin              Turin  45.07049    7.68682            Italy\n",
      "17           Florence           Florence  43.77925   11.24626            Italy\n",
      "18  Alcalá de Henares  Alcalá de Henares  40.48205   -3.35996            Spain\n",
      "19             Leiden             Leiden  52.15833    4.49306  The Netherlands\n",
      "\n",
      "Places that were NOT found by Geonames (sample):\n",
      "['Liegnitz, Wrocław' 'Stockholm, Uppsala, Turku' 'Leipzig, Desau'\n",
      " 'Pitschen [Byczyna], Oels [Oleśnica]' 'Leipzig, Wolfenbüttel'\n",
      " 'Lyon, Zurich' 'nan' 'Bratislava, Komorn'\n",
      " 'Nuremberg, Frankfurt am Main, Leipzig' 'Vienna, London, Leiden'\n",
      " 'Żagań, Frankfurt am Main' 'Linz, Frankfurt am Main' 'not indicated'\n",
      " 'Leiden, Stockholm, Erlangen' 'Leipzig, Copenhagen'\n",
      " 'Philadelphia (fictive)' 'Leiden, Amsterdam' 'Augsburg, Innsbruck'\n",
      " 'Venice [Modena]' 'Neostadii in Palatinate (Neustadt an der Weinstraße)']\n",
      "Total unique raw places not found: 23\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Geocode Raw Publication Places\n",
    "\n",
    "GEONAMES_USERNAME = \"utaysi\"  # Your Geonames username\n",
    "raw_geocoded_cache_file = 'raw_geocoded_places_cache.csv'\n",
    "\n",
    "def get_coordinates(place_name, username):\n",
    "    if not place_name or pd.isna(place_name):\n",
    "        return None, None, None, None\n",
    "    # Ensure place_name is a string for requests.utils.quote\n",
    "    place_name_str = str(place_name)\n",
    "    try:\n",
    "        # Initial attempt: prioritize populated places (featureClass=P)\n",
    "        url = f\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&featureClass=P&username={username}\"\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        data = response.json()\n",
    "        if data.get('geonames') and len(data['geonames']) > 0:\n",
    "            top_result = data['geonames'][0]\n",
    "            return float(top_result['lat']), float(top_result['lng']), top_result.get('name'), top_result.get('countryName')\n",
    "        else:\n",
    "            # Fallback: search without featureClass if no populated place found or if initial result is empty\n",
    "            # This helps with broader terms or historical names that might not be classed as 'P'\n",
    "            url_fallback = f\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&username={username}\"\n",
    "            # print(f\"Retrying without featureClass for: {place_name_str}\") # Optional: for debugging\n",
    "            response_fallback = requests.get(url_fallback, timeout=15)\n",
    "            response_fallback.raise_for_status()\n",
    "            data_fallback = response_fallback.json()\n",
    "            if data_fallback.get('geonames') and len(data_fallback['geonames']) > 0:\n",
    "                top_result_fallback = data_fallback['geonames'][0]\n",
    "                # print(f\"Fallback success for {place_name_str}: Found {top_result_fallback.get('name')}\") # Optional\n",
    "                return float(top_result_fallback['lat']), float(top_result_fallback['lng']), top_result_fallback.get('name'), top_result_fallback.get('countryName')\n",
    "            # print(f\"Place not found by Geonames (even after fallback): {place_name_str}\") # Optional\n",
    "            return None, None, None, None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"API request timed out for {place_name_str}\")\n",
    "        return None, None, None, None\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred for {place_name_str}: {http_err} - Response: {response.text[:200]}...\")\n",
    "        return None, None, None, None\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"API request failed for {place_name_str}: {req_err}\")\n",
    "        return None, None, None, None\n",
    "    except ValueError as json_err: # Handles JSON decoding errors\n",
    "        print(f\"JSON decoding failed for {place_name_str} (response: {response.text[:200]}...): {json_err}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Check for cached data first\n",
    "if os.path.exists(raw_geocoded_cache_file):\n",
    "    print(f\"Loading raw geocoded data from cache: {raw_geocoded_cache_file}\")\n",
    "    raw_geocoded_df = pd.read_csv(raw_geocoded_cache_file)\n",
    "    # Ensure all expected columns are present, fill with NA if not\n",
    "    expected_cols = ['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']\n",
    "    for col in expected_cols:\n",
    "        if col not in raw_geocoded_df.columns:\n",
    "            raw_geocoded_df[col] = pd.NA\n",
    "else:\n",
    "    print(f\"No cache file found ({raw_geocoded_cache_file}). Geocoding raw places...\")\n",
    "    raw_geocoded_data = []\n",
    "    if 'places_series' in locals():\n",
    "        unique_raw_places = places_series.dropna().unique() # Use dropna() before unique()\n",
    "        print(f\"Geocoding {len(unique_raw_places)} unique raw place names...\")\n",
    "        for i, place in enumerate(unique_raw_places):\n",
    "            if str(place).strip() == \"nan\" or str(place).strip() == \"\": # Skip if place is 'nan' string or empty after strip\n",
    "                # print(f\"Skipping invalid place entry: '{place}'\") # Optional\n",
    "                lat, lon, geoname_name, country = None, None, None, None\n",
    "            else:\n",
    "                if (i+1) % 20 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(unique_raw_places)} places...\")\n",
    "                lat, lon, geoname_name, country = get_coordinates(place, GEONAMES_USERNAME)\n",
    "            \n",
    "            raw_geocoded_data.append({'raw_place': place, \n",
    "                                      'geoname_name': geoname_name, \n",
    "                                      'latitude': lat, \n",
    "                                      'longitude': lon, \n",
    "                                      'country': country})\n",
    "            time.sleep(0.1) # 100ms delay to be respectful to the API\n",
    "\n",
    "        raw_geocoded_df = pd.DataFrame(raw_geocoded_data)\n",
    "        raw_geocoded_df.to_csv(raw_geocoded_cache_file, index=False)\n",
    "        print(f\"Saved raw geocoded data to cache: {raw_geocoded_cache_file}\")\n",
    "    else:\n",
    "        print(\"Error: 'places_series' not defined. Please ensure the previous cells (especially 'cline_extract_place_column') have been run.\")\n",
    "        raw_geocoded_df = pd.DataFrame(columns=['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']) # Create empty df\n",
    "\n",
    "if not raw_geocoded_df.empty:\n",
    "    print(f\"\\nSuccessfully geocoded {raw_geocoded_df['latitude'].notna().sum()} places out of {len(raw_geocoded_df)} unique raw names processed.\")\n",
    "    print(\"\\nSample of geocoded data (first 20 rows):\")\n",
    "    print(raw_geocoded_df.head(20))\n",
    "    \n",
    "    print(\"\\nPlaces that were NOT found by Geonames (sample):\")\n",
    "    not_found_sample = raw_geocoded_df[raw_geocoded_df['latitude'].isna()]['raw_place'].unique()\n",
    "    print(not_found_sample[:20]) # Show up to 20 unique not found raw places\n",
    "    print(f\"Total unique raw places not found: {len(not_found_sample)}\")\n",
    "else:\n",
    "    print(\"\\nraw_geocoded_df is empty. Check for errors in previous steps or API calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cline_conditional_cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 unique raw place names that were not geocoded in Phase 2.\n",
      "\n",
      "Applying cleaning function to failed place names...\n",
      "\n",
      "Generated 17 unique cleaned names from the 23 failed raw names.\n",
      "Sample of cleaned names to be retried for geocoding (first 30):\n",
      "['Aga', 'Augsburg', 'Bratislava', 'Frankfurt Am Main', 'Leiden', 'Leipzig', 'Liegnitz', 'Linz', 'Lyon', 'Neostadii In Palatinate', 'Nuremberg', 'Paris', 'Philadelphia', 'Pitschen', 'Stockholm', 'Venice', 'Vienna']\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Analyze Geocoding Results and Conditional Cleaning\n",
    "\n",
    "# Ensure raw_geocoded_df exists from the previous step\n",
    "if 'raw_geocoded_df' in locals() and not raw_geocoded_df.empty:\n",
    "    failed_raw_places_df = raw_geocoded_df[raw_geocoded_df['latitude'].isna()]\n",
    "    unique_failed_raw_places = failed_raw_places_df['raw_place'].dropna().unique().tolist()\n",
    "    print(f\"Found {len(unique_failed_raw_places)} unique raw place names that were not geocoded in Phase 2.\")\n",
    "    # print(\"Sample of failed raw places:\", unique_failed_raw_places[:20]) # Optional: for debugging\n",
    "\n",
    "    # Define the cleaning function (similar to what was planned before, now applied conditionally)\n",
    "    def clean_place_name(name):\n",
    "        name_str = str(name).lower().strip() # Ensure string, lowercase, strip whitespace\n",
    "        name_str = re.sub(r\"\\(.*?\\)\", \"\", name_str).strip() # Remove content in parentheses\n",
    "        name_str = re.sub(r\"\\[.*?\\]\", \"\", name_str).strip() # Remove content in brackets\n",
    "        replacements = {\n",
    "            \"lvgduni batavorvm\": \"leiden\", \"lugduni batavorum\": \"leiden\",\n",
    "            \"amstelodami\": \"amsterdam\", \"amstelædami\": \"amsterdam\", \"amstelodamum\": \"amsterdam\",\n",
    "            \"parisiis\": \"paris\", \"londini\": \"london\", \"franequerae\": \"franeker\",\n",
    "            \"hafniae\": \"copenhagen\", \"coloniae agrippinae\": \"cologne\",\n",
    "            \"antverpiae\": \"antwerp\", \"lipsiae\": \"leipzig\", \"argentorati\": \"strasbourg\",\n",
    "            \"s.l.\": \"\", \"s. l.\": \"\", \"s.a.\": \"\", \"s.n.\": \"\", \"o.o.\": \"\", \"o. o.\": \"\",\n",
    "            \"not indicated\": \"\", \"nan\": \"\"\n",
    "        }\n",
    "        name_str = replacements.get(name_str, name_str)\n",
    "        if ',' in name_str:\n",
    "            parts = re.split(r'[,;/&]', name_str)\n",
    "            name_str = parts[0].strip()\n",
    "            name_str = replacements.get(name_str, name_str)\n",
    "        name_str = re.sub(r\"[^a-z\\s'’ʻ-]\", \"\", name_str, flags=re.UNICODE).strip()\n",
    "        name_str = re.sub(r\"\\s+\", \" \", name_str).strip()\n",
    "        return name_str.title() if name_str else \"\"\n",
    "\n",
    "    if unique_failed_raw_places:\n",
    "        print(\"\\nApplying cleaning function to failed place names...\")\n",
    "        cleaned_failed_places = sorted(list(set([clean_place_name(p) for p in unique_failed_raw_places])))\n",
    "        cleaned_failed_places = [p for p in cleaned_failed_places if p] # Remove empty strings after cleaning\n",
    "        print(f\"\\nGenerated {len(cleaned_failed_places)} unique cleaned names from the {len(unique_failed_raw_places)} failed raw names.\")\n",
    "        print(\"Sample of cleaned names to be retried for geocoding (first 30):\")\n",
    "        print(cleaned_failed_places[:30])\n",
    "        # Store for next phase\n",
    "        places_to_retry_geocoding = cleaned_failed_places\n",
    "    else:\n",
    "        print(\"\\nNo failed place names to clean or retry.\")\n",
    "        places_to_retry_geocoding = []\n",
    "else:\n",
    "    print(\"Error: 'raw_geocoded_df' not found or is empty. Please run Phase 2 (cline_geocode_raw_places) first.\")\n",
    "    places_to_retry_geocoding = [] # Initialize to prevent errors in next phase if this one fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371283c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
