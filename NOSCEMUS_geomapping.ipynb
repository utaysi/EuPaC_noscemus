{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0A: Setup - Install Libraries\n",
    "%pip install folium geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b2bdaa3269606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:17:07.109340Z",
     "start_time": "2025-05-01T14:17:07.105530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Phase 0B: Setup - Import Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1A: Load Dataset\n",
    "# Display 2 sample DataFrame rows\n",
    "noscemus_metadata = pd.read_csv(\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\")\n",
    "noscemus_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1B: Inspect DataFrame Structure\n",
    "# Display DataFrame Columns\n",
    "\n",
    "print(\"\\nColumns in noscemus_metadata:\")\n",
    "print(noscemus_metadata.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1C: Examine 'Place' Column\n",
    "# Inspect Potential Columns\n",
    "# Replace 'candidate_column_name' with a column name from the list above\n",
    "candidate_column_name = 'Place' # <-- CHANGE THIS VALUE \n",
    "\n",
    "if candidate_column_name in noscemus_metadata.columns:\n",
    "    print(f\"\\nUnique values in '{candidate_column_name}':\")\n",
    "    # Display a sample of unique values and their counts\n",
    "    print(noscemus_metadata[candidate_column_name].value_counts().head(30))\n",
    "    print(f\"\\nNumber of unique values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].nunique()}\")\n",
    "    print(f\"Number of missing values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].isnull().sum()}\")\n",
    "    # Show some raw examples of the data in this column\n",
    "    print(\"\\nSample raw entries (up to first 20 non-null):\")\n",
    "    print(noscemus_metadata[candidate_column_name].dropna().head(20).tolist())\n",
    "else:\n",
    "    print(f\"Column '{candidate_column_name}' not found in DataFrame. Please choose from the list printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_expand_multi_location_rows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2A: Define Place Splitting Logic and Expand Rows\n",
    "\n",
    "def split_places(place_string):\n",
    "    if pd.isna(place_string) or not isinstance(place_string, str):\n",
    "        return [] # Return empty list for NaN or non-string input\n",
    "    # Split by comma, semicolon, or ampersand. Also handle cases like 'Place1 / Place2'.\n",
    "    # Regex looks for one or more delimiters, surrounded by optional whitespace.\n",
    "    places = re.split(r'\\s*[,;&/]\\s*', place_string)\n",
    "    # Clean up each individual place name: strip whitespace, remove empty strings\n",
    "    return [p.strip() for p in places if p and p.strip()] \n",
    "\n",
    "expanded_rows = []\n",
    "if 'noscemus_metadata' in locals():\n",
    "    print(f\"Original number of rows in noscemus_metadata: {len(noscemus_metadata)}\")\n",
    "    for index, row in noscemus_metadata.iterrows():\n",
    "        original_place_entry = row['Place']\n",
    "        individual_places = split_places(original_place_entry)\n",
    "        \n",
    "        if not individual_places: # Handles NaN, empty strings, or strings that become empty after split\n",
    "            # Keep the row as is, but ensure 'Place' is None or a consistent empty marker if it was NaN/empty\n",
    "            new_row = row.copy()\n",
    "            new_row['Place'] = None # Or np.nan, or an empty string, depending on desired handling for mapping\n",
    "            expanded_rows.append(new_row)\n",
    "        elif len(individual_places) == 1:\n",
    "            # Single place, just copy the row with the cleaned single place name\n",
    "            new_row = row.copy()\n",
    "            new_row['Place'] = individual_places[0]\n",
    "            expanded_rows.append(new_row)\n",
    "        else:\n",
    "            # Multiple places, create a new row for each\n",
    "            for place_name in individual_places:\n",
    "                new_row = row.copy()\n",
    "                new_row['Place'] = place_name\n",
    "                # Add original multi-place string for reference if needed\n",
    "                new_row['Original_Multi_Place_Entry'] = original_place_entry \n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    expanded_noscemus_metadata = pd.DataFrame(expanded_rows)\n",
    "    print(f\"Number of rows after expansion: {len(expanded_noscemus_metadata)}\")\n",
    "\n",
    "    # Display a sample, especially focusing on some known multi-place entries to verify\n",
    "    print(\"\\nSample of expanded_noscemus_metadata (showing some original multi-place entries):\")\n",
    "    # Example: Find rows originating from 'Liegnitz, Wrocław' if it exists\n",
    "    if 'Original_Multi_Place_Entry' in expanded_noscemus_metadata.columns:\n",
    "        sample_multi = expanded_noscemus_metadata[expanded_noscemus_metadata['Original_Multi_Place_Entry'] == 'Liegnitz, Wrocław']\n",
    "        if not sample_multi.empty:\n",
    "            print(sample_multi[['id', 'Full title', 'Place', 'Original_Multi_Place_Entry']].head())\n",
    "        else:\n",
    "            print(\"Could not find 'Liegnitz, Wrocław' in Original_Multi_Place_Entry for sample.\")\n",
    "        # Show general head as well\n",
    "        print(\"\\nGeneral head of expanded data:\")\n",
    "        print(expanded_noscemus_metadata[['id', 'Full title', 'Place', 'Original_Multi_Place_Entry' if 'Original_Multi_Place_Entry' in expanded_noscemus_metadata.columns else 'Place']].head())\n",
    "    else:\n",
    "        print(\"\\nGeneral head of expanded data (Original_Multi_Place_Entry column not created, likely no multi-place entries found):\")\n",
    "        print(expanded_noscemus_metadata[['id', 'Full title', 'Place']].head())\n",
    "else:\n",
    "    print(\"Error: noscemus_metadata DataFrame not found. Please load it first.\")\n",
    "    expanded_noscemus_metadata = pd.DataFrame() # Initialize empty to avoid errors later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_extract_place_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2B: Extract Unique Place Names for Geocoding\n",
    "# Ensure 'expanded_noscemus_metadata' is available and populated from the previous cell.\n",
    "if 'expanded_noscemus_metadata' in locals() and not expanded_noscemus_metadata.empty:\n",
    "    actual_publication_place_column = 'Place' # This is the column with individual place names\n",
    "    places_series = expanded_noscemus_metadata[actual_publication_place_column].astype(str).str.strip()\n",
    "    unique_raw_places = places_series.dropna().unique() # Important to dropna here\n",
    "    print(f\"Found {len(unique_raw_places)} unique raw place mentions from '{actual_publication_place_column}' in the expanded data.\")\n",
    "    print(\"Sample of raw places (first 50 from expanded data):\")\n",
    "    print(unique_raw_places[:50])\n",
    "else:\n",
    "    print(\"Error: expanded_noscemus_metadata is not available or empty. Please ensure the 'Expand Multi-Location Rows' cell ran successfully.\")\n",
    "    # Initialize empty to prevent errors in subsequent cells, or handle appropriately\n",
    "    places_series = pd.Series(dtype=str) \n",
    "    unique_raw_places = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_geocode_raw_places",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3A: Geocode Unique Place Names\n",
    "\n",
    "GEONAMES_USERNAME = \"utaysi\"  # Your Geonames username\n",
    "raw_geocoded_cache_file = 'raw_geocoded_places_cache.csv'\n",
    "\n",
    "def get_coordinates(place_name, username):\n",
    "    if not place_name or pd.isna(place_name):\n",
    "        return None, None, None, None\n",
    "    # Ensure place_name is a string for requests.utils.quote\n",
    "    place_name_str = str(place_name)\n",
    "    try:\n",
    "        # Initial attempt: prioritize populated places (featureClass=P)\n",
    "        url = f\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&featureClass=P&username={username}\"\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        data = response.json()\n",
    "        if data.get('geonames') and len(data['geonames']) > 0:\n",
    "            top_result = data['geonames'][0]\n",
    "            return float(top_result['lat']), float(top_result['lng']), top_result.get('name'), top_result.get('countryName')\n",
    "        else:\n",
    "            # Fallback: search without featureClass if no populated place found or if initial result is empty\n",
    "            # This helps with broader terms or historical names that might not be classed as 'P'\n",
    "            url_fallback = f\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&username={username}\"\n",
    "            # print(f\"Retrying without featureClass for: {place_name_str}\") # Optional: for debugging\n",
    "            response_fallback = requests.get(url_fallback, timeout=15)\n",
    "            response_fallback.raise_for_status()\n",
    "            data_fallback = response_fallback.json()\n",
    "            if data_fallback.get('geonames') and len(data_fallback['geonames']) > 0:\n",
    "                top_result_fallback = data_fallback['geonames'][0]\n",
    "                # print(f\"Fallback success for {place_name_str}: Found {top_result_fallback.get('name')}\") # Optional\n",
    "                return float(top_result_fallback['lat']), float(top_result_fallback['lng']), top_result_fallback.get('name'), top_result_fallback.get('countryName')\n",
    "            # print(f\"Place not found by Geonames (even after fallback): {place_name_str}\") # Optional\n",
    "            return None, None, None, None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"API request timed out for {place_name_str}\")\n",
    "        return None, None, None, None\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred for {place_name_str}: {http_err} - Response: {response.text[:200]}...\")\n",
    "        return None, None, None, None\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"API request failed for {place_name_str}: {req_err}\")\n",
    "        return None, None, None, None\n",
    "    except ValueError as json_err: # Handles JSON decoding errors\n",
    "        print(f\"JSON decoding failed for {place_name_str} (response: {response.text[:200]}...): {json_err}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Check for cached data first\n",
    "if os.path.exists(raw_geocoded_cache_file):\n",
    "    print(f\"Loading raw geocoded data from cache: {raw_geocoded_cache_file}\")\n",
    "    raw_geocoded_df = pd.read_csv(raw_geocoded_cache_file)\n",
    "    # Ensure all expected columns are present, fill with NA if not\n",
    "    expected_cols = ['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']\n",
    "    for col in expected_cols:\n",
    "        if col not in raw_geocoded_df.columns:\n",
    "            raw_geocoded_df[col] = pd.NA\n",
    "else:\n",
    "    print(f\"No cache file found ({raw_geocoded_cache_file}). Geocoding raw places...\")\n",
    "    raw_geocoded_data = []\n",
    "    if 'places_series' in locals():\n",
    "        unique_raw_places = places_series.dropna().unique() # Use dropna() before unique()\n",
    "        print(f\"Geocoding {len(unique_raw_places)} unique raw place names...\")\n",
    "        for i, place in enumerate(unique_raw_places):\n",
    "            if str(place).strip() == \"nan\" or str(place).strip() == \"\": # Skip if place is 'nan' string or empty after strip\n",
    "                # print(f\"Skipping invalid place entry: '{place}'\") # Optional\n",
    "                lat, lon, geoname_name, country = None, None, None, None\n",
    "            else:\n",
    "                if (i+1) % 20 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(unique_raw_places)} places...\")\n",
    "                lat, lon, geoname_name, country = get_coordinates(place, GEONAMES_USERNAME)\n",
    "            \n",
    "            raw_geocoded_data.append({'raw_place': place, \n",
    "                                      'geoname_name': geoname_name, \n",
    "                                      'latitude': lat, \n",
    "                                      'longitude': lon, \n",
    "                                      'country': country})\n",
    "            time.sleep(0.1) # 100ms delay to be respectful to the API\n",
    "\n",
    "        raw_geocoded_df = pd.DataFrame(raw_geocoded_data)\n",
    "        raw_geocoded_df.to_csv(raw_geocoded_cache_file, index=False)\n",
    "        print(f\"Saved raw geocoded data to cache: {raw_geocoded_cache_file}\")\n",
    "    else:\n",
    "        print(\"Error: 'places_series' not defined. Please ensure the previous cells (especially 'cline_extract_place_column') have been run.\")\n",
    "        raw_geocoded_df = pd.DataFrame(columns=['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']) # Create empty df\n",
    "\n",
    "if not raw_geocoded_df.empty:\n",
    "    print(f\"\\nSuccessfully geocoded {raw_geocoded_df['latitude'].notna().sum()} places out of {len(raw_geocoded_df)} unique raw names processed.\")\n",
    "    print(\"\\nSample of geocoded data (first 20 rows):\")\n",
    "    print(raw_geocoded_df.head(20))\n",
    "    \n",
    "    print(\"\\nPlaces that were NOT found by Geonames (sample):\")\n",
    "    not_found_sample = raw_geocoded_df[raw_geocoded_df['latitude'].isna()]['raw_place'].unique()\n",
    "    print(not_found_sample[:20]) # Show up to 20 unique not found raw places\n",
    "    print(f\"Total unique raw places not found: {len(not_found_sample)}\")\n",
    "else:\n",
    "    print(\"\\nraw_geocoded_df is empty. Check for errors in previous steps or API calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"6bafbaa7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Requirement already satisfied: folium in ./.venv/lib/python3.12/site-packages (0.19.6)\\n\",\n",
    "      \"Requirement already satisfied: geopandas in ./.venv/lib/python3.12/site-packages (1.0.1)\\n\",\n",
    "      \"Requirement already satisfied: branca>=0.6.0 in ./.venv/lib/python3.12/site-packages (from folium) (0.8.1)\\n\",\n",
    "      \"Requirement already satisfied: jinja2>=2.9 in ./.venv/lib/python3.12/site-packages (from folium) (3.1.6)\\n\",\n",
    "      \"Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from folium) (1.26.4)\\n\",\n",
    "      \"Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from folium) (2.32.3)\\n\",\n",
    "      \"Requirement already satisfied: xyzservices in ./.venv/lib/python3.12/site-packages (from folium) (2025.4.0)\\n\",\n",
    "      \"Requirement already satisfied: pyogrio>=0.7.2 in ./.venv/lib/python3.12/site-packages (from geopandas) (0.11.0)\\n\",\n",
    "      \"Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from geopandas) (25.0)\\n\",\n",
    "      \"Requirement already satisfied: pandas>=1.4.0 in ./.venv/lib/python3.12/site-packages (from geopandas) (2.2.3)\\n\",\n",
    "      \"Requirement already satisfied: pyproj>=3.3.0 in ./.venv/lib/python3.12/site-packages (from geopandas) (3.7.1)\\n\",\n",
    "      \"Requirement already satisfied: shapely>=2.0.0 in ./.venv/lib/python3.12/site-packages (from geopandas) (2.1.1)\\n\",\n",
    "      \"Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2>=2.9->folium) (3.0.2)\\n\",\n",
    "      \"Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\\n\",\n",
    "      \"Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2025.2)\\n\",\n",
    "      \"Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2025.2)\\n\",\n",
    "      \"Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.4.26)\\n\",\n",
    "      \"Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\\n\",\n",
    "      \"Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->folium) (3.4.2)\\n\",\n",
    "      \"Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->folium) (3.10)\\n\",\n",
    "      \"Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->folium) (2.4.0)\\n\",\n",
    "      \"Note: you may need to restart the kernel to use updated packages.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 0A: Setup - Install Libraries\\n\",\n",
    "    \"%pip install folium geopandas\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"361b2bdaa3269606\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-05-01T14:17:07.109340Z\",\n",
    "     \"start_time\": \"2025-05-01T14:17:07.105530Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Phase 0B: Setup - Import Libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import io\\n\",\n",
    "    \"import folium\\n\",\n",
    "    \"import geopandas as gpd\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import time\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"9741c876\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Author</th>\\n\",\n",
    "       \"      <th>Full title</th>\\n\",\n",
    "       \"      <th>In</th>\\n\",\n",
    "       \"      <th>Year</th>\\n\",\n",
    "       \"      <th>Place</th>\\n\",\n",
    "       \"      <th>Publisher/Printer</th>\\n\",\n",
    "       \"      <th>Era</th>\\n\",\n",
    "       \"      <th>Form/Genre</th>\\n\",\n",
    "       \"      <th>Discipline/Content</th>\\n\",\n",
    "       \"      <th>Original</th>\\n\",\n",
    "       \"      <th>...</th>\\n\",\n",
    "       \"      <th>Of interest to</th>\\n\",\n",
    "       \"      <th>Transkribus text available</th>\\n\",\n",
    "       \"      <th>Written by</th>\\n\",\n",
    "       \"      <th>Library and Signature</th>\\n\",\n",
    "       \"      <th>ids</th>\\n\",\n",
    "       \"      <th>id</th>\\n\",\n",
    "       \"      <th>date_min</th>\\n\",\n",
    "       \"      <th>date_max</th>\\n\",\n",
    "       \"      <th>filename</th>\\n\",\n",
    "       \"      <th>file_year</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Achrelius, Daniel</td>\\n\",\n",
    "       \"      <td>Scientiarum magnes recitatus publice anno 1690...</td>\\n\",\n",
    "       \"      <td>NaN</td>\\n\",\n",
    "       \"      <td>1690</td>\\n\",\n",
    "       \"      <td>[Turku]</td>\\n\",\n",
    "       \"      <td>Wall</td>\\n\",\n",
    "       \"      <td>17th century</td>\\n\",\n",
    "       \"      <td>Oration</td>\\n\",\n",
    "       \"      <td>Mathematics, Astronomy/Astrology/Cosmography, ...</td>\\n\",\n",
    "       \"      <td>Scientiarum magnes(Google Books)</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>MK, JL</td>\\n\",\n",
    "       \"      <td>Yes</td>\\n\",\n",
    "       \"      <td>IT</td>\\n\",\n",
    "       \"      <td>NaN</td>\\n\",\n",
    "       \"      <td>[705665]</td>\\n\",\n",
    "       \"      <td>705665</td>\\n\",\n",
    "       \"      <td>1690.0</td>\\n\",\n",
    "       \"      <td>1690.0</td>\\n\",\n",
    "       \"      <td>Achrelius,_Daniel_-_Scientiarum_magnes__Turku_...</td>\\n\",\n",
    "       \"      <td>1690.0</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Acidalius, Valens</td>\\n\",\n",
    "       \"      <td>Ad Iordanum Brunum Nolanum, Italum</td>\\n\",\n",
    "       \"      <td>Poematum Iani Lernutii, Iani Gulielmi, Valenti...</td>\\n\",\n",
    "       \"      <td>1603</td>\\n\",\n",
    "       \"      <td>Liegnitz, Wrocław</td>\\n\",\n",
    "       \"      <td>Albert, David</td>\\n\",\n",
    "       \"      <td>17th century</td>\\n\",\n",
    "       \"      <td>Panegyric poem</td>\\n\",\n",
    "       \"      <td>Astronomy/Astrology/Cosmography</td>\\n\",\n",
    "       \"      <td>Ad Iordanum Brunum (1603)(CAMENA)Ad Iordanum B...</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>MK, IT</td>\\n\",\n",
    "       \"      <td>Yes</td>\\n\",\n",
    "       \"      <td>MK</td>\\n\",\n",
    "       \"      <td>NaN</td>\\n\",\n",
    "       \"      <td>[801745]</td>\\n\",\n",
    "       \"      <td>801745</td>\\n\",\n",
    "       \"      <td>1603.0</td>\\n\",\n",
    "       \"      <td>1603.0</td>\\n\",\n",
    "       \"      <td>Janus_Lernutius_et_al__-_Poemata__Liegnitz_160...</td>\\n\",\n",
    "       \"      <td>1603.0</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"<p>2 rows × 26 columns</p>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"              Author                                         Full title  \\\\\\n\",\n",
    "       \"0  Achrelius, Daniel  Scientiarum magnes recitatus publice anno 1690...   \\n\",\n",
    "       \"1  Acidalius, Valens                 Ad Iordanum Brunum Nolanum, Italum   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                  In  Year              Place  \\\\\\n\",\n",
    "       \"0                                                NaN  1690            [Turku]   \\n\",\n",
    "       \"1  Poematum Iani Lernutii, Iani Gulielmi, Valenti...  1603  Liegnitz, Wrocław   \\n\",\n",
    "       \"\\n\",\n",
    "       \"  Publisher/Printer           Era      Form/Genre  \\\\\\n\",\n",
    "       \"0              Wall  17th century         Oration   \\n\",\n",
    "       \"1     Albert, David  17th century  Panegyric poem   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                  Discipline/Content  \\\\\\n\",\n",
    "       \"0  Mathematics, Astronomy/Astrology/Cosmography, ...   \\n\",\n",
    "       \"1                    Astronomy/Astrology/Cosmography   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                            Original  ... Of interest to  \\\\\\n\",\n",
    "       \"0                   Scientiarum magnes(Google Books)  ...         MK, JL   \\n\",\n",
    "       \"1  Ad Iordanum Brunum (1603)(CAMENA)Ad Iordanum B...  ...         MK, IT   \\n\",\n",
    "       \"\\n\",\n",
    "       \"  Transkribus text available Written by Library and Signature       ids  \\\\\\n\",\n",
    "       \"0                        Yes         IT                   NaN  [705665]   \\n\",\n",
    "       \"1                        Yes         MK                   NaN  [801745]   \\n\",\n",
    "       \"\\n\",\n",
    "       \"       id date_min date_max  \\\\\\n\",\n",
    "       \"0  705665   1690.0   1690.0   \\n\",\n",
    "       \"1  801745   1603.0   1603.0   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                            filename file_year  \\n\",\n",
    "       \"0  Achrelius,_Daniel_-_Scientiarum_magnes__Turku_...    1690.0  \\n\",\n",
    "       \"1  Janus_Lernutius_et_al__-_Poemata__Liegnitz_160...    1603.0  \\n\",\n",
    "       \"\\n\",\n",
    "       \"[2 rows x 26 columns]\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 3,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 1A: Data Exploration\\n\",\n",
    "    \"# Display 2 sample DataFrame rows\\n\",\n",
    "    \"noscemus_metadata = pd.read_csv(\\\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\\\")\\n\",\n",
    "    \"noscemus_metadata.head(2)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"ca89376f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\",\n",
    "      \"Columns in noscemus_metadata:\\n\",\n",
    "      \"['Author', 'Full title', 'In', 'Year', 'Place', 'Publisher/Printer', 'Era', 'Form/Genre', 'Discipline/Content', 'Original', 'Digital sourcebook', 'Description', 'References', 'Cited in', 'How to cite this entry', 'Internal notes', 'Of interest to', 'Transkribus text available', 'Written by', 'Library and Signature', 'ids', 'id', 'date_min', 'date_max', 'filename', 'file_year']\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 1B: Data Exploration\\n\",\n",
    "    \"# Display DataFrame Columns\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nColumns in noscemus_metadata:\\\")\\n\",\n",
    "    \"print(noscemus_metadata.columns.tolist())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"id\": \"13a6aa9a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\",\n",
    "      \"Unique values in 'Place':\\n\",\n",
    "      \"Place\\n\",\n",
    "      \"Paris                          69\\n\",\n",
    "      \"Amsterdam                      49\\n\",\n",
    "      \"Basel                          48\\n\",\n",
    "      \"Venice                         48\\n\",\n",
    "      \"London                         40\\n\",\n",
    "      \"Leipzig                        36\\n\",\n",
    "      \"Rome                           34\\n\",\n",
    "      \"Zurich                         33\\n\",\n",
    "      \"Leiden                         29\\n\",\n",
    "      \"Frankfurt am Main              26\\n\",\n",
    "      \"Göttingen                      25\\n\",\n",
    "      \"Tübingen                       25\\n\",\n",
    "      \"Nuremberg                      21\\n\",\n",
    "      \"Bologna                        21\\n\",\n",
    "      \"Strasbourg                     20\\n\",\n",
    "      \"Lyon                           19\\n\",\n",
    "      \"Wittenberg                     17\\n\",\n",
    "      \"Innsbruck                      16\\n\",\n",
    "      \"Cologne                        13\\n\",\n",
    "      \"Padua                          13\\n\",\n",
    "      \"Naples                         12\\n\",\n",
    "      \"Florence                       12\\n\",\n",
    "      \"Leiden, Stockholm, Erlangen    10\\n\",\n",
    "      \"Halle                          10\\n\",\n",
    "      \"Antwerp                        10\\n\",\n",
    "      \"Oxford                          8\\n\",\n",
    "      \"Copenhagen                      8\\n\",\n",
    "      \"Vienna                          8\\n\",\n",
    "      \"Bern                            7\\n\",\n",
    "      \"Augsburg                        7\\n\",\n",
    "      \"Name: count, dtype: int64\\n\",\n",
    "      \"\\n\",\n",
    "      \"Number of unique values in 'Place': 173\\n\",\n",
    "      \"Number of missing values in 'Place': 4\\n\",\n",
    "      \"\\n\",\n",
    "      \"Sample raw entries (up to first 20 non-null):\\n\",\n",
    "      \"['[Turku]', 'Liegnitz, Wrocław', 'Salamanca', 'Heidelberg', 'London', 'Oxford', 'Lund', 'Strasbourg', 'Basel', 'Basel', 'Basel', 'Basel', 'Basel', 'Bologna', 'Leipzig', 'Zurich', 'Venice', 'Rome', 'Herborn', 'Frankfurt am Main']\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 1C: Data Exploration\\n\",\n",
    "    \"# Inspect Potential Columns\\n\",\n",
    "    \"# Replace 'candidate_column_name' with a column name from the list above\\n\",\n",
    "    \"candidate_column_name = 'Place' # <-- CHANGE THIS VALUE \\n\",\n",
    "    \"\\n\",\n",
    "    \"if candidate_column_name in noscemus_metadata.columns:\\n\",\n",
    "    \"    print(f\\\"\\\\nUnique values in '{candidate_column_name}':\\\")\\n\",\n",
    "    \"    # Display a sample of unique values and their counts\\n\",\n",
    "    \"    print(noscemus_metadata[candidate_column_name].value_counts().head(30))\\n\",\n",
    "    \"    print(f\\\"\\\\nNumber of unique values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].nunique()}\\\")\\n\",\n",
    "    \"    print(f\\\"Number of missing values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].isnull().sum()}\\\")\\n\",\n",
    "    \"    # Show some raw examples of the data in this column\\n\",\n",
    "    \"    print(\\\"\\\\nSample raw entries (up to first 20 non-null):\\\")\\n\",\n",
    "    \"    print(noscemus_metadata[candidate_column_name].dropna().head(20).tolist())\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Column '{candidate_column_name}' not found in DataFrame. Please choose from the list printed above.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"id\": \"cline_expand_multi_location_rows\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Original number of rows in noscemus_metadata: 975\\n\",\n",
    "      \"Number of rows after expansion: 1030\\n\",\n",
    "      \"\\n\",\n",
    "      \"Sample of expanded_noscemus_metadata (showing some original multi-place entries):\\n\",\n",
    "      \"       id                          Full title     Place  \\\\\\n\",\n",
    "      \"1  801745  Ad Iordanum Brunum Nolanum, Italum  Liegnitz   \\n\",\n",
    "      \"1  801745  Ad Iordanum Brunum Nolanum, Italum   Wrocław   \\n\",\n",
    "      \"\\n\",\n",
    "      \"  Original_Multi_Place_Entry  \\n\",\n",
    "      \"1          Liegnitz, Wrocław  \\n\",\n",
    "      \"1          Liegnitz, Wrocław  \\n\",\n",
    "      \"\\n\",\n",
    "      \"General head of expanded data:\\n\",\n",
    "      \"       id                                         Full title       Place  \\\\\\n\",\n",
    "      \"0  705665  Scientiarum magnes recitatus publice anno 1690...     [Turku]   \\n\",\n",
    "      \"1  801745                 Ad Iordanum Brunum Nolanum, Italum    Liegnitz   \\n\",\n",
    "      \"1  801745                 Ad Iordanum Brunum Nolanum, Italum     Wrocław   \\n\",\n",
    "      \"2  713323  De natura novi orbis libri duo et De promulgat...   Salamanca   \\n\",\n",
    "      \"3  693148  Vitae Germanorum medicorum, qui saeculo superi...  Heidelberg   \\n\",\n",
    "      \"\\n\",\n",
    "      \"  Original_Multi_Place_Entry  \\n\",\n",
    "      \"0                        NaN  \\n\",\n",
    "      \"1          Liegnitz, Wrocław  \\n\",\n",
    "      \"1          Liegnitz, Wrocław  \\n\",\n",
    "      \"2                        NaN  \\n\",\n",
    "      \"3                        NaN  \\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 1 (New Plan): Expand Multi-Location Rows\\n\",\n",
    "    \"\\n\",\n",
    "    \"def split_places(place_string):\\n\",\n",
    "    \"    if pd.isna(place_string) or not isinstance(place_string, str):\\n\",\n",
    "    \"        return [] # Return empty list for NaN or non-string input\\n\",\n",
    "    \"    # Split by comma, semicolon, or ampersand. Also handle cases like 'Place1 / Place2'.\\n\",\n",
    "    \"    # Regex looks for one or more delimiters, surrounded by optional whitespace.\\n\",\n",
    "    \"    places = re.split(r'\\\\s*[,;&/]\\\\s*', place_string)\\n\",\n",
    "    \"    # Clean up each individual place name: strip whitespace, remove empty strings\\n\",\n",
    "    \"    return [p.strip() for p in places if p and p.strip()] \\n\",\n",
    "    \"\\n\",\n",
    "    \"expanded_rows = []\\n\",\n",
    "    \"if 'noscemus_metadata' in locals():\\n\",\n",
    "    \"    print(f\\\"Original number of rows in noscemus_metadata: {len(noscemus_metadata)}\\\")\\n\",\n",
    "    \"    for index, row in noscemus_metadata.iterrows():\\n\",\n",
    "    \"        original_place_entry = row['Place']\\n\",\n",
    "    \"        individual_places = split_places(original_place_entry)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if not individual_places: # Handles NaN, empty strings, or strings that become empty after split\\n\",\n",
    "    \"            # Keep the row as is, but ensure 'Place' is None or a consistent empty marker if it was NaN/empty\\n\",\n",
    "    \"            new_row = row.copy()\\n\",\n",
    "    \"            new_row['Place'] = None # Or np.nan, or an empty string, depending on desired handling for mapping\\n\",\n",
    "    \"            expanded_rows.append(new_row)\\n\",\n",
    "    \"        elif len(individual_places) == 1:\\n\",\n",
    "    \"            # Single place, just copy the row with the cleaned single place name\\n\",\n",
    "    \"            new_row = row.copy()\\n\",\n",
    "    \"            new_row['Place'] = individual_places[0]\\n\",\n",
    "    \"            expanded_rows.append(new_row)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Multiple places, create a new row for each\\n\",\n",
    "    \"            for place_name in individual_places:\\n\",\n",
    "    \"                new_row = row.copy()\\n\",\n",
    "    \"                new_row['Place'] = place_name\\n\",\n",
    "    \"                # Add original multi-place string for reference if needed\\n\",\n",
    "    \"                new_row['Original_Multi_Place_Entry'] = original_place_entry \\n\",\n",
    "    \"                expanded_rows.append(new_row)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    expanded_noscemus_metadata = pd.DataFrame(expanded_rows)\\n\",\n",
    "    \"    print(f\\\"Number of rows after expansion: {len(expanded_noscemus_metadata)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Display a sample, especially focusing on some known multi-place entries to verify\\n\",\n",
    "    \"    print(\\\"\\\\nSample of expanded_noscemus_metadata (showing some original multi-place entries):\\\")\\n\",\n",
    "    \"    # Example: Find rows originating from 'Liegnitz, Wrocław' if it exists\\n\",\n",
    "    \"    if 'Original_Multi_Place_Entry' in expanded_noscemus_metadata.columns:\\n\",\n",
    "    \"        sample_multi = expanded_noscemus_metadata[expanded_noscemus_metadata['Original_Multi_Place_Entry'] == 'Liegnitz, Wrocław']\\n\",\n",
    "    \"        if not sample_multi.empty:\\n\",\n",
    "    \"            print(sample_multi[['id', 'Full title', 'Place', 'Original_Multi_Place_Entry']].head())\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(\\\"Could not find 'Liegnitz, Wrocław' in Original_Multi_Place_Entry for sample.\\\")\\n\",\n",
    "    \"        # Show general head as well\\n\",\n",
    "    \"        print(\\\"\\\\nGeneral head of expanded data:\\\")\\n\",\n",
    "    \"        print(expanded_noscemus_metadata[['id', 'Full title', 'Place', 'Original_Multi_Place_Entry' if 'Original_Multi_Place_Entry' in expanded_noscemus_metadata.columns else 'Place']].head())\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"\\\\nGeneral head of expanded data (Original_Multi_Place_Entry column not created, likely no multi-place entries found):\\\")\\n\",\n",
    "    \"        print(expanded_noscemus_metadata[['id', 'Full title', 'Place']].head())\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Error: noscemus_metadata DataFrame not found. Please load it first.\\\")\\n\",\n",
    "    \"    expanded_noscemus_metadata = pd.DataFrame() # Initialize empty to avoid errors later\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"id\": \"cline_extract_place_column\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Found 166 unique raw place mentions from 'Place' in the expanded data.\\n\",\n",
    "      \"Sample of raw places (first 50 from expanded data):\\n\",\n",
    "      \"['[Turku]' 'Liegnitz' 'Wrocław' 'Salamanca' 'Heidelberg' 'London' 'Oxford'\\n\",\n",
    "      \" 'Lund' 'Strasbourg' 'Basel' 'Bologna' 'Leipzig' 'Zurich' 'Venice' 'Rome'\\n\",\n",
    "      \" 'Herborn' 'Frankfurt am Main' 'Turin' 'Florence' 'Alcalá de Henares'\\n\",\n",
    "      \" 'Leiden' 'Innsbruck' 'Westminster Abbey' 'Paris' 'Cambridge' '[Landshut]'\\n\",\n",
    "      \" '[Ingolstadt]' 'Milan' 'Bergamo' 'Stuttgart' 'Perugia' 'Lyon' 's.l.'\\n\",\n",
    "      \" 'Amsterdam' '[Wittenberg]' 'Copenhagen' 'Padua' '[Padua]' 'Rimini'\\n\",\n",
    "      \" 'Büdingen' 'Königsberg' 'Uppsala' 'Stockholm' 'Turku' 'Desau' 'Würzburg'\\n\",\n",
    "      \" 'Saint Petersburg' 'Antwerp' 'Graz' 'Aachen']\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 1: Data Extraction - Extract 'Place' column\\n\",\n",
    "    \"# Ensure 'expanded_noscemus_metadata' is available and populated from the previous cell.\\n\",\n",
    "    \"if 'expanded_noscemus_metadata' in locals() and not expanded_noscemus_metadata.empty:\\n\",\n",
    "    \"    actual_publication_place_column = 'Place' # This is the column with individual place names\\n\",\n",
    "    \"    places_series = expanded_noscemus_metadata[actual_publication_place_column].astype(str).str.strip()\\n\",\n",
    "    \"    unique_raw_places = places_series.dropna().unique() # Important to dropna here\\n\",\n",
    "    \"    print(f\\\"Found {len(unique_raw_places)} unique raw place mentions from '{actual_publication_place_column}' in the expanded data.\\\")\\n\",\n",
    "    \"    print(\\\"Sample of raw places (first 50 from expanded data):\\\")\\n\",\n",
    "    \"    print(unique_raw_places[:50])\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Error: expanded_noscemus_metadata is not available or empty. Please ensure the 'Expand Multi-Location Rows' cell ran successfully.\\\")\\n\",\n",
    "    \"    # Initialize empty to prevent errors in subsequent cells, or handle appropriately\\n\",\n",
    "    \"    places_series = pd.Series(dtype=str) \\n\",\n",
    "    \"    unique_raw_places = []\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"id\": \"cline_geocode_raw_places\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"No cache file found (raw_geocoded_places_cache.csv). Geocoding raw places...\\n\",\n",
    "      \"Geocoding 166 unique raw place names...\\n\",\n",
    "      \"Processed 20/166 places...\\n\",\n",
    "      \"Processed 40/166 places...\\n\",\n",
    "      \"Processed 60/166 places...\\n\",\n",
    "      \"Processed 80/166 places...\\n\",\n",
    "      \"Processed 100/166 places...\\n\",\n",
    "      \"Processed 120/166 places...\\n\",\n",
    "      \"Processed 140/166 places...\\n\",\n",
    "      \"Processed 160/166 places...\\n\",\n",
    "      \"Saved raw geocoded data to cache: raw_geocoded_places_cache.csv\\n\",\n",
    "      \"\\n\",\n",
    "      \"Successfully geocoded 162 places out of 166 unique raw names processed.\\n\",\n",
    "      \"\\n\",\n",
    "      \"Sample of geocoded data (first 20 rows):\\n\",\n",
    "      \"            raw_place       geoname_name  latitude  longitude         country\\n\",\n",
    "      \"0             [Turku]              Turku  60.45148   22.26869         Finland\\n\",\n",
    "      \"1            Liegnitz            Legnica  51.21006   16.16190          Poland\\n\",\n",
    "      \"2             Wrocław            Wroclaw  51.10000   17.03333          Poland\\n\",\n",
    "      \"3           Salamanca          Salamanca  40.96882   -5.66388           Spain\\n\",\n",
    "      \"4          Heidelberg         Heidelberg  49.40768    8.69079         Germany\\n\",\n",
    "      \"5              London             London  51.50853   -0.12574  United Kingdom\\n\",\n",
    "      \"6              Oxford             Oxford  39.50700  -84.74523   United States\\n\",\n",
    "      \"7                Lund               Lund  55.70584   13.19321          Sweden\\n\",\n",
    "      \"8          Strasbourg         Strasbourg  48.58392    7.74553          France\\n\",\n",
    "      \"9               Basel              Basel  47.55839    7.57327     Switzerland\\n\",\n",
    "      \"10            Bologna            Bologna  44.49381   11.33875           Italy\\n\",\n",
    "      \"11            Leipzig            Leipzig  51.33962   12.37129         Germany\\n\",\n",
    "      \"12             Zurich             Zurich  47.36667    8.55000     Switzerland\\n\",\n",
    "      \"13             Venice             Venice  45.43713   12.33265           Italy\\n\",\n",
    "      \"14               Rome               Rome  41.89193   12.51133           Italy\\n\",\n",
    "      \"15            Herborn            Herborn  49.74167    6.42778      Luxembourg\\n\",\n",
    "      \"16  Frankfurt am Main  Frankfurt am Main  50.11552    8.68417         Germany\\n\",\n",
    "      \"17              Turin              Turin  45.07049    7.68682           Italy\\n\",\n",
    "      \"18           Florence           Florence  43.77925   11.24626           Italy\\n\",\n",
    "      \"19  Alcalá de Henares  Alcalá de Henares  40.48205   -3.35996           Spain\\n\",\n",
    "      \"\\n\",\n",
    "      \"Places that were NOT found by Geonames (sample):\\n\",\n",
    "      \"['not indicated' 'Philadelphia (fictive)' 'Venice [Modena]'\\n\",\n",
    "      \" 'Neostadii in Palatinate (Neustadt an der Weinstraße)']\\n\",\n",
    "      \"Total unique raw places not found: 4\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Phase 2: Geocode Raw Publication Places\\n\",\n",
    "    \"\\n\",\n",
    "    \"GEONAMES_USERNAME = \\\"utaysi\\\"  # Your Geonames username\\n\",\n",
    "    \"raw_geocoded_cache_file = 'raw_geocoded_places_cache.csv'\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_coordinates(place_name, username):\\n\",\n",
    "    \"    if not place_name or pd.isna(place_name):\\n\",\n",
    "    \"        return None, None, None, None\\n\",\n",
    "    \"    # Ensure place_name is a string for requests.utils.quote\\n\",\n",
    "    \"    place_name_str = str(place_name)\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Initial attempt: prioritize populated places (featureClass=P)\\n\",\n",
    "    \"        url = f\\\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&featureClass=P&username={username}\\\"\\n\",\n",
    "    \"        response = requests.get(url, timeout=15)\\n\",\n",
    "    \"        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\\n\",\n",
    "    \"        data = response.json()\\n\",\n",
    "    \"        if data.get('geonames') and len(data['geonames']) > 0:\\n\",\n",
    "    \"            top_result = data['geonames'][0]\\n\",\n",
    "    \"            return float(top_result['lat']), float(top_result['lng']), top_result.get('name'), top_result.get('countryName')\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Fallback: search without featureClass if no populated place found or if initial result is empty\\n\",\n",
    "    \"            # This helps with broader terms or historical names that might not be classed as 'P'\\n\",\n",
    "    \"            url_fallback = f\\\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&username={username}\\\"\\n\",\n",
    "    \"            # print(f\\\"Retrying without featureClass for: {place_name_str}\\\") # Optional: for debugging\\n\",\n",
    "    \"            response_fallback = requests.get(url_fallback, timeout=15)\\n\",\n",
    "    \"            response_fallback.raise_for_status()\\n\",\n",
    "    \"            data_fallback = response_fallback.json()\\n\",\n",
    "    \"            if data_fallback.get('geonames') and len(data_fallback['geonames']) > 0:\\n\",\n",
    "    \"                top_result_fallback = data_fallback['geonames'][0]\\n\",\n",
    "    \"                # print(f\\\"Fallback success for {place_name_str}: Found {top_result_fallback.get('name')}\\\") # Optional\\n\",\n",
    "    \"                return float(top_result_fallback['lat']), float(top_result_fallback['lng']), top_result_fallback.get('name'), top_result_fallback.get('countryName')\\n\",\n",
    "    \"            # print(f\\\"Place not found by Geonames (even after fallback): {place_name_str}\\\") # Optional\\n\",\n",
    "    \"            return None, None, None, None\\n\",\n",
    "    \"    except requests.exceptions.Timeout:\\n\",\n",
    "    \"        print(f\\\"API request timed out for {place_name_str}\\\")\\n\",\n",
    "    \"        return None, None, None, None\\n\",\n",
    "    \"    except requests.exceptions.HTTPError as http_err:\\n\",\n",
    "    \"        print(f\\\"HTTP error occurred for {place_name_str}: {http_err} - Response: {response.text[:200]}...\\\")\\n\",\n",
    "    \"        return None, None, None, None\\n\",\n",
    "    \"    except requests.exceptions.RequestException as req_err:\\n\",\n",
    "    \"        print(f\\\"API request failed for {place_name_str}: {req_err}\\\")\\n\",\n",
    "    \"        return None, None, None, None\\n\",\n",
    "    \"    except ValueError as json_err: # Handles JSON decoding errors\\n\",\n",
    "    \"        print(f\\\"JSON decoding failed for {place_name_str} (response: {response.text[:200]}...): {json_err}\\\")\\n\",\n",
    "    \"        return None, None, None, None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check for cached data first\\n\",\n",
    "    \"if os.path.exists(raw_geocoded_cache_file):\\n\",\n",
    "    \"    print(f\\\"Loading raw geocoded data from cache: {raw_geocoded_cache_file}\\\")\\n\",\n",
    "    \"    raw_geocoded_df = pd.read_csv(raw_geocoded_cache_file)\\n\",\n",
    "    \"    # Ensure all expected columns are present, fill with NA if not\\n\",\n",
    "    \"    expected_cols = ['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']\\n\",\n",
    "    \"    for col in expected_cols:\\n\",\n",
    "    \"        if col not in raw_geocoded_df.columns:\\n\",\n",
    "    \"            raw_geocoded_df[col] = pd.NA\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"No cache file found ({raw_geocoded_cache_file}). Geocoding raw places...\\\")\\n\",\n",
    "    \"    raw_geocoded_data = []\\n\",\n",
    "    \"    if 'places_series' in locals():\\n\",\n",
    "    \"        unique_raw_places = places_series.dropna().unique() # Use dropna() before unique()\\n\",\n",
    "    \"        print(f\\\"Geocoding {len(unique_raw_places)} unique raw place names...\\\")\\n\",\n",
    "    \"        for i, place in enumerate(unique_raw_places):\\n\",\n",
    "    \"            if str(place).strip() == \\\"nan\\\" or str(place).strip() == \\\"\\\": # Skip if place is 'nan' string or empty after strip\\n\",\n",
    "    \"                # print(f\\\"Skipping invalid place entry: '{place}'\\\") # Optional\\n\",\n",
    "    \"                lat, lon, geoname_name, country = None, None, None, None\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                if (i+1) % 20 == 0:\\n\",\n",
    "    \"                    print(f\\\"Processed {i+1}/{len(unique_raw_places)} places...\\\")\\n\",\n",
    "    \"                lat, lon, geoname_name, country = get_coordinates(place, GEONAMES_USERNAME)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            raw_geocoded_data.append({'raw_place': place, \\n\",\n",
    "    \"                                      'geoname_name': geoname_name, \\n\",\n",
    "    \"                                      'latitude': lat, \\n\",\n",
    "    \"                                      'longitude': lon, \\n\",\n",
    "    \"                                      'country': country})\\n\",\n",
    "    \"            time.sleep(0.1) # 100ms delay to be respectful to the API\\n\",\n",
    "    \"\\n\",\n",
    "    \"        raw_geocoded_df = pd.DataFrame(raw_geocoded_data)\\n\",\n",
    "    \"        raw_geocoded_df.to_csv(raw_geocoded_cache_file, index=False)\\n\",\n",
    "    \"        print(f\\\"Saved raw geocoded data to cache: {raw_geocoded_cache_file}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"Error: 'places_series' not defined. Please ensure the previous cells (especially 'cline_extract_place_column') have been run.\\\")\\n\",\n",
    "    \"        raw_geocoded_df = pd.DataFrame(columns=['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']) # Create empty df\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not raw_geocoded_df.empty:\\n\",\n",
    "    \"    print(f\\\"\\\\nSuccessfully geocoded {raw_geocoded_df['latitude'].notna().sum()} places out of {len(raw_geocoded_df)} unique raw names processed.\\\")\\n\",\n",
    "    \"    print(\\\"\\\\nSample of geocoded data (first 20 rows):\\\")\\n\",\n",
    "    \"    print(raw_geocoded_df.head(20))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nPlaces that were NOT found by Geonames (sample):\\\")\\n\",\n",
    "    \"    not_found_sample = raw_geocoded_df[raw_geocoded_df['latitude'].isna()]['raw_place'].unique()\\n\",\n",
    "    \"    print(not_found_sample[:20]) # Show up to 20 unique not found raw places\\n\",\n",
    "    \"    print(f\\\"Total unique raw places not found: {len(not_found_sample)}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nraw_geocoded_df is empty. Check for errors in previous steps or API calls.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"6002ab86\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \".venv\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
