{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0A: Setup - Install Libraries\n",
    "%pip install folium geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b2bdaa3269606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:17:07.109340Z",
     "start_time": "2025-05-01T14:17:07.105530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Phase 0B: Setup - Import Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1A: Load Dataset\n",
    "# Display 2 sample DataFrame rows\n",
    "noscemus_metadata = pd.read_csv(\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\")\n",
    "noscemus_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1B: Inspect DataFrame Structure\n",
    "# Display DataFrame Columns\n",
    "\n",
    "print(\"\\nColumns in noscemus_metadata:\")\n",
    "print(noscemus_metadata.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1C: Examine 'Place' Column\n",
    "# Inspect Potential Columns\n",
    "# Replace 'candidate_column_name' with a column name from the list above\n",
    "candidate_column_name = 'Place' # <-- CHANGE THIS VALUE \n",
    "\n",
    "if candidate_column_name in noscemus_metadata.columns:\n",
    "    print(f\"\\nUnique values in '{candidate_column_name}':\")\n",
    "    # Display a sample of unique values and their counts\n",
    "    print(noscemus_metadata[candidate_column_name].value_counts().head(30))\n",
    "    print(f\"\\nNumber of unique values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].nunique()}\")\n",
    "    print(f\"Number of missing values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].isnull().sum()}\")\n",
    "    # Show some raw examples of the data in this column\n",
    "    print(\"\\nSample raw entries (up to first 20 non-null):\")\n",
    "    print(noscemus_metadata[candidate_column_name].dropna().head(20).tolist())\n",
    "else:\n",
    "    print(f\"Column '{candidate_column_name}' not found in DataFrame. Please choose from the list printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_expand_multi_location_rows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2A: Define Place Splitting Logic and Expand Rows\n",
    "\n",
    "def split_places(place_string):\n",
    "    if pd.isna(place_string) or not isinstance(place_string, str):\n",
    "        return [] # Return empty list for NaN or non-string input\n",
    "    # Split by comma, semicolon, or ampersand. Also handle cases like 'Place1 / Place2'.\n",
    "    # Regex looks for one or more delimiters, surrounded by optional whitespace.\n",
    "    places = re.split(r'\\s*[,;&/]\\s*', place_string)\n",
    "    # Clean up each individual place name: strip whitespace, remove empty strings\n",
    "    return [p.strip() for p in places if p and p.strip()] \n",
    "\n",
    "expanded_rows = []\n",
    "if 'noscemus_metadata' in locals():\n",
    "    print(f\"Original number of rows in noscemus_metadata: {len(noscemus_metadata)}\")\n",
    "    for index, row in noscemus_metadata.iterrows():\n",
    "        original_place_entry = row['Place']\n",
    "        individual_places = split_places(original_place_entry)\n",
    "        \n",
    "        if not individual_places: # Handles NaN, empty strings, or strings that become empty after split\n",
    "            # Keep the row as is, but ensure 'Place' is None or a consistent empty marker if it was NaN/empty\n",
    "            new_row = row.copy()\n",
    "            new_row['Place'] = None # Or np.nan, or an empty string, depending on desired handling for mapping\n",
    "            expanded_rows.append(new_row)\n",
    "        elif len(individual_places) == 1:\n",
    "            # Single place, just copy the row with the cleaned single place name\n",
    "            new_row = row.copy()\n",
    "            new_row['Place'] = individual_places[0]\n",
    "            expanded_rows.append(new_row)\n",
    "        else:\n",
    "            # Multiple places, create a new row for each\n",
    "            for place_name in individual_places:\n",
    "                new_row = row.copy()\n",
    "                new_row['Place'] = place_name\n",
    "                # Add original multi-place string for reference if needed\n",
    "                new_row['Original_Multi_Place_Entry'] = original_place_entry \n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    expanded_noscemus_metadata = pd.DataFrame(expanded_rows)\n",
    "    print(f\"Number of rows after expansion: {len(expanded_noscemus_metadata)}\")\n",
    "\n",
    "    # Display a sample, especially focusing on some known multi-place entries to verify\n",
    "    print(\"\\nSample of expanded_noscemus_metadata (showing some original multi-place entries):\")\n",
    "    # Example: Find rows originating from 'Liegnitz, Wrocław' if it exists\n",
    "    if 'Original_Multi_Place_Entry' in expanded_noscemus_metadata.columns:\n",
    "        sample_multi = expanded_noscemus_metadata[expanded_noscemus_metadata['Original_Multi_Place_Entry'] == 'Liegnitz, Wrocław']\n",
    "        if not sample_multi.empty:\n",
    "            print(sample_multi[['id', 'Full title', 'Place', 'Original_Multi_Place_Entry']].head())\n",
    "        else:\n",
    "            print(\"Could not find 'Liegnitz, Wrocław' in Original_Multi_Place_Entry for sample.\")\n",
    "        # Show general head as well\n",
    "        print(\"\\nGeneral head of expanded data:\")\n",
    "        print(expanded_noscemus_metadata[['id', 'Full title', 'Place', 'Original_Multi_Place_Entry' if 'Original_Multi_Place_Entry' in expanded_noscemus_metadata.columns else 'Place']].head())\n",
    "    else:\n",
    "        print(\"\\nGeneral head of expanded data (Original_Multi_Place_Entry column not created, likely no multi-place entries found):\")\n",
    "        print(expanded_noscemus_metadata[['id', 'Full title', 'Place']].head())\n",
    "else:\n",
    "    print(\"Error: noscemus_metadata DataFrame not found. Please load it first.\")\n",
    "    expanded_noscemus_metadata = pd.DataFrame() # Initialize empty to avoid errors later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_extract_place_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2B: Extract Unique Place Names for Geocoding\n",
    "# Ensure 'expanded_noscemus_metadata' is available and populated from the previous cell.\n",
    "if 'expanded_noscemus_metadata' in locals() and not expanded_noscemus_metadata.empty:\n",
    "    actual_publication_place_column = 'Place' # This is the column with individual place names\n",
    "    places_series = expanded_noscemus_metadata[actual_publication_place_column].astype(str).str.strip()\n",
    "    unique_raw_places = places_series.dropna().unique() # Important to dropna here\n",
    "    print(f\"Found {len(unique_raw_places)} unique raw place mentions from '{actual_publication_place_column}' in the expanded data.\")\n",
    "    print(\"Sample of raw places (first 50 from expanded data):\")\n",
    "    print(unique_raw_places[:50])\n",
    "else:\n",
    "    print(\"Error: expanded_noscemus_metadata is not available or empty. Please ensure the 'Expand Multi-Location Rows' cell ran successfully.\")\n",
    "    # Initialize empty to prevent errors in subsequent cells, or handle appropriately\n",
    "    places_series = pd.Series(dtype=str) \n",
    "    unique_raw_places = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_geocode_raw_places",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3A: Geocode Unique Place Names\n",
    "\n",
    "GEONAMES_USERNAME = \"utaysi\"  # Your Geonames username\n",
    "raw_geocoded_cache_file = 'raw_geocoded_places_cache.csv'\n",
    "\n",
    "def get_coordinates(place_name, username):\n",
    "    if not place_name or pd.isna(place_name):\n",
    "        return None, None, None, None\n",
    "    # Ensure place_name is a string for requests.utils.quote\n",
    "    place_name_str = str(place_name)\n",
    "    try:\n",
    "        # Initial attempt: prioritize populated places (featureClass=P)\n",
    "        url = f\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&featureClass=P&username={username}\"\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        data = response.json()\n",
    "        if data.get('geonames') and len(data['geonames']) > 0:\n",
    "            top_result = data['geonames'][0]\n",
    "            return float(top_result['lat']), float(top_result['lng']), top_result.get('name'), top_result.get('countryName')\n",
    "        else:\n",
    "            # Fallback: search without featureClass if no populated place found or if initial result is empty\n",
    "            # This helps with broader terms or historical names that might not be classed as 'P'\n",
    "            url_fallback = f\"http://api.geonames.org/searchJSON?q={requests.utils.quote(place_name_str)}&maxRows=1&username={username}\"\n",
    "            # print(f\"Retrying without featureClass for: {place_name_str}\") # Optional: for debugging\n",
    "            response_fallback = requests.get(url_fallback, timeout=15)\n",
    "            response_fallback.raise_for_status()\n",
    "            data_fallback = response_fallback.json()\n",
    "            if data_fallback.get('geonames') and len(data_fallback['geonames']) > 0:\n",
    "                top_result_fallback = data_fallback['geonames'][0]\n",
    "                # print(f\"Fallback success for {place_name_str}: Found {top_result_fallback.get('name')}\") # Optional\n",
    "                return float(top_result_fallback['lat']), float(top_result_fallback['lng']), top_result_fallback.get('name'), top_result_fallback.get('countryName')\n",
    "            # print(f\"Place not found by Geonames (even after fallback): {place_name_str}\") # Optional\n",
    "            return None, None, None, None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"API request timed out for {place_name_str}\")\n",
    "        return None, None, None, None\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred for {place_name_str}: {http_err} - Response: {response.text[:200]}...\")\n",
    "        return None, None, None, None\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"API request failed for {place_name_str}: {req_err}\")\n",
    "        return None, None, None, None\n",
    "    except ValueError as json_err: # Handles JSON decoding errors\n",
    "        print(f\"JSON decoding failed for {place_name_str} (response: {response.text[:200]}...): {json_err}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Check for cached data first\n",
    "if os.path.exists(raw_geocoded_cache_file):\n",
    "    print(f\"Loading raw geocoded data from cache: {raw_geocoded_cache_file}\")\n",
    "    raw_geocoded_df = pd.read_csv(raw_geocoded_cache_file)\n",
    "    # Ensure all expected columns are present, fill with NA if not\n",
    "    expected_cols = ['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']\n",
    "    for col in expected_cols:\n",
    "        if col not in raw_geocoded_df.columns:\n",
    "            raw_geocoded_df[col] = pd.NA\n",
    "else:\n",
    "    print(f\"No cache file found ({raw_geocoded_cache_file}). Geocoding raw places...\")\n",
    "    raw_geocoded_data = []\n",
    "    if 'places_series' in locals():\n",
    "        unique_raw_places = places_series.dropna().unique() # Use dropna() before unique()\n",
    "        print(f\"Geocoding {len(unique_raw_places)} unique raw place names...\")\n",
    "        for i, place in enumerate(unique_raw_places):\n",
    "            if str(place).strip() == \"nan\" or str(place).strip() == \"\": # Skip if place is 'nan' string or empty after strip\n",
    "                # print(f\"Skipping invalid place entry: '{place}'\") # Optional\n",
    "                lat, lon, geoname_name, country = None, None, None, None\n",
    "            else:\n",
    "                if (i+1) % 20 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(unique_raw_places)} places...\")\n",
    "                lat, lon, geoname_name, country = get_coordinates(place, GEONAMES_USERNAME)\n",
    "            \n",
    "            raw_geocoded_data.append({'raw_place': place, \n",
    "                                      'geoname_name': geoname_name, \n",
    "                                      'latitude': lat, \n",
    "                                      'longitude': lon, \n",
    "                                      'country': country})\n",
    "            time.sleep(0.1) # 100ms delay to be respectful to the API\n",
    "\n",
    "        raw_geocoded_df = pd.DataFrame(raw_geocoded_data)\n",
    "        raw_geocoded_df.to_csv(raw_geocoded_cache_file, index=False)\n",
    "        print(f\"Saved raw geocoded data to cache: {raw_geocoded_cache_file}\")\n",
    "    else:\n",
    "        print(\"Error: 'places_series' not defined. Please ensure the previous cells (especially 'cline_extract_place_column') have been run.\")\n",
    "        raw_geocoded_df = pd.DataFrame(columns=['raw_place', 'geoname_name', 'latitude', 'longitude', 'country']) # Create empty df\n",
    "\n",
    "if not raw_geocoded_df.empty:\n",
    "    print(f\"\\nSuccessfully geocoded {raw_geocoded_df['latitude'].notna().sum()} places out of {len(raw_geocoded_df)} unique raw names processed.\")\n",
    "    print(\"\\nSample of geocoded data (first 20 rows):\")\n",
    "    print(raw_geocoded_df.head(20))\n",
    "    \n",
    "    print(\"\\nPlaces that were NOT found by Geonames (sample):\")\n",
    "    not_found_sample = raw_geocoded_df[raw_geocoded_df['latitude'].isna()]['raw_place'].unique()\n",
    "    print(not_found_sample[:20]) # Show up to 20 unique not found raw places\n",
    "    print(f\"Total unique raw places not found: {len(not_found_sample)}\")\n",
    "else:\n",
    "    print(\"\\nraw_geocoded_df is empty. Check for errors in previous steps or API calls.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
