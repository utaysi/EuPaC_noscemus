{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b001cbcc97cb66",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CCS-ZCU/EuPaC_shared/blob/master/NOSCEMUS_getting-started.ipynb)\n",
    "\n",
    "This Jupyter notebook has been prepared for the EuPaC Hackathon and provides an easy way to start working with the NOSCEMUS dataset — no need to clone the entire repository or download additional data. It is fully compatible with cloud platforms like Google Colaboratory (click the badge above) and runs without requiring any specialized library installations.\n",
    "\n",
    "As such, it is intended as a starting point for EuPaC participants, including those with minimal coding experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b2bdaa3269606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:17:07.109340Z",
     "start_time": "2025-05-01T14:17:07.105530Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cline_import_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Setup - Import Libraries\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f256c66aa1cbbdbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:08.472459Z",
     "start_time": "2025-05-01T14:52:07.967666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Full title</th>\n",
       "      <th>In</th>\n",
       "      <th>Year</th>\n",
       "      <th>Place</th>\n",
       "      <th>Publisher/Printer</th>\n",
       "      <th>Era</th>\n",
       "      <th>Form/Genre</th>\n",
       "      <th>Discipline/Content</th>\n",
       "      <th>Original</th>\n",
       "      <th>...</th>\n",
       "      <th>Of interest to</th>\n",
       "      <th>Transkribus text available</th>\n",
       "      <th>Written by</th>\n",
       "      <th>Library and Signature</th>\n",
       "      <th>ids</th>\n",
       "      <th>id</th>\n",
       "      <th>date_min</th>\n",
       "      <th>date_max</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Achrelius, Daniel</td>\n",
       "      <td>Scientiarum magnes recitatus publice anno 1690...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1690</td>\n",
       "      <td>[Turku]</td>\n",
       "      <td>Wall</td>\n",
       "      <td>17th century</td>\n",
       "      <td>Oration</td>\n",
       "      <td>Mathematics, Astronomy/Astrology/Cosmography, ...</td>\n",
       "      <td>Scientiarum magnes(Google Books)</td>\n",
       "      <td>...</td>\n",
       "      <td>MK, JL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>IT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[705665]</td>\n",
       "      <td>705665</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>Achrelius,_Daniel_-_Scientiarum_magnes__Turku_...</td>\n",
       "      <td>1690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acidalius, Valens</td>\n",
       "      <td>Ad Iordanum Brunum Nolanum, Italum</td>\n",
       "      <td>Poematum Iani Lernutii, Iani Gulielmi, Valenti...</td>\n",
       "      <td>1603</td>\n",
       "      <td>Liegnitz, Wrocław</td>\n",
       "      <td>Albert, David</td>\n",
       "      <td>17th century</td>\n",
       "      <td>Panegyric poem</td>\n",
       "      <td>Astronomy/Astrology/Cosmography</td>\n",
       "      <td>Ad Iordanum Brunum (1603)(CAMENA)Ad Iordanum B...</td>\n",
       "      <td>...</td>\n",
       "      <td>MK, IT</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[801745]</td>\n",
       "      <td>801745</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>Janus_Lernutius_et_al__-_Poemata__Liegnitz_160...</td>\n",
       "      <td>1603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acosta, José de</td>\n",
       "      <td>De natura novi orbis libri duo et De promulgat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1589</td>\n",
       "      <td>Salamanca</td>\n",
       "      <td>Guillelum Foquel</td>\n",
       "      <td>16th century</td>\n",
       "      <td>Monograph</td>\n",
       "      <td>Astronomy/Astrology/Cosmography, Geography/Car...</td>\n",
       "      <td>De natura novi orbis(Biodiversity Heritage Lib...</td>\n",
       "      <td>...</td>\n",
       "      <td>DB</td>\n",
       "      <td>Yes</td>\n",
       "      <td>DB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[713323]</td>\n",
       "      <td>713323</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>Acosta,_José_de_-_De_natura_novi_orbis__Salama...</td>\n",
       "      <td>1589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam, Melchior</td>\n",
       "      <td>Vitae Germanorum medicorum, qui saeculo superi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1620</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>Rosa, Geyder</td>\n",
       "      <td>17th century</td>\n",
       "      <td>Biography</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Vitae Germanorum medicorum(MDZ)Alternative lin...</td>\n",
       "      <td>...</td>\n",
       "      <td>IT</td>\n",
       "      <td>Yes</td>\n",
       "      <td>IT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[693148]</td>\n",
       "      <td>693148</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>Adam,_Melchior_-_Vitae_Germanorum_medicorum__H...</td>\n",
       "      <td>1620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Addison, Joseph</td>\n",
       "      <td>Ad insignissimum virum dominum Thomam Burnettu...</td>\n",
       "      <td>Examen poeticum duplex, sive, Musarum anglican...</td>\n",
       "      <td>1698</td>\n",
       "      <td>London</td>\n",
       "      <td>Richard Wellington I.</td>\n",
       "      <td>17th century</td>\n",
       "      <td>Panegyric poem</td>\n",
       "      <td>Meteorology/Earth sciences</td>\n",
       "      <td>Ad Burnettum sacrae theoriae telluris auctorem...</td>\n",
       "      <td>...</td>\n",
       "      <td>MK, IT</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[769230]</td>\n",
       "      <td>769230</td>\n",
       "      <td>1698.0</td>\n",
       "      <td>1698.0</td>\n",
       "      <td>Examen_poeticum_duplex__London_1698_pdf.txt</td>\n",
       "      <td>1698.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                         Full title  \\\n",
       "0  Achrelius, Daniel  Scientiarum magnes recitatus publice anno 1690...   \n",
       "1  Acidalius, Valens                 Ad Iordanum Brunum Nolanum, Italum   \n",
       "2    Acosta, José de  De natura novi orbis libri duo et De promulgat...   \n",
       "3     Adam, Melchior  Vitae Germanorum medicorum, qui saeculo superi...   \n",
       "4    Addison, Joseph  Ad insignissimum virum dominum Thomam Burnettu...   \n",
       "\n",
       "                                                  In  Year              Place  \\\n",
       "0                                                NaN  1690            [Turku]   \n",
       "1  Poematum Iani Lernutii, Iani Gulielmi, Valenti...  1603  Liegnitz, Wrocław   \n",
       "2                                                NaN  1589          Salamanca   \n",
       "3                                                NaN  1620         Heidelberg   \n",
       "4  Examen poeticum duplex, sive, Musarum anglican...  1698             London   \n",
       "\n",
       "       Publisher/Printer           Era      Form/Genre  \\\n",
       "0                   Wall  17th century         Oration   \n",
       "1          Albert, David  17th century  Panegyric poem   \n",
       "2       Guillelum Foquel  16th century       Monograph   \n",
       "3           Rosa, Geyder  17th century       Biography   \n",
       "4  Richard Wellington I.  17th century  Panegyric poem   \n",
       "\n",
       "                                  Discipline/Content  \\\n",
       "0  Mathematics, Astronomy/Astrology/Cosmography, ...   \n",
       "1                    Astronomy/Astrology/Cosmography   \n",
       "2  Astronomy/Astrology/Cosmography, Geography/Car...   \n",
       "3                                           Medicine   \n",
       "4                         Meteorology/Earth sciences   \n",
       "\n",
       "                                            Original  ... Of interest to  \\\n",
       "0                   Scientiarum magnes(Google Books)  ...         MK, JL   \n",
       "1  Ad Iordanum Brunum (1603)(CAMENA)Ad Iordanum B...  ...         MK, IT   \n",
       "2  De natura novi orbis(Biodiversity Heritage Lib...  ...             DB   \n",
       "3  Vitae Germanorum medicorum(MDZ)Alternative lin...  ...             IT   \n",
       "4  Ad Burnettum sacrae theoriae telluris auctorem...  ...         MK, IT   \n",
       "\n",
       "  Transkribus text available Written by Library and Signature       ids  \\\n",
       "0                        Yes         IT                   NaN  [705665]   \n",
       "1                        Yes         MK                   NaN  [801745]   \n",
       "2                        Yes         DB                   NaN  [713323]   \n",
       "3                        Yes         IT                   NaN  [693148]   \n",
       "4                        Yes         MK                   NaN  [769230]   \n",
       "\n",
       "       id date_min date_max  \\\n",
       "0  705665   1690.0   1690.0   \n",
       "1  801745   1603.0   1603.0   \n",
       "2  713323   1589.0   1589.0   \n",
       "3  693148   1620.0   1620.0   \n",
       "4  769230   1698.0   1698.0   \n",
       "\n",
       "                                            filename file_year  \n",
       "0  Achrelius,_Daniel_-_Scientiarum_magnes__Turku_...    1690.0  \n",
       "1  Janus_Lernutius_et_al__-_Poemata__Liegnitz_160...    1603.0  \n",
       "2  Acosta,_José_de_-_De_natura_novi_orbis__Salama...    1589.0  \n",
       "3  Adam,_Melchior_-_Vitae_Germanorum_medicorum__H...    1620.0  \n",
       "4        Examen_poeticum_duplex__London_1698_pdf.txt    1698.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noscemus_metadata = pd.read_csv(\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\")\n",
    "noscemus_metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cline_inserted_cell_cols",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in noscemus_metadata:\n",
      "['Author', 'Full title', 'In', 'Year', 'Place', 'Publisher/Printer', 'Era', 'Form/Genre', 'Discipline/Content', 'Original', 'Digital sourcebook', 'Description', 'References', 'Cited in', 'How to cite this entry', 'Internal notes', 'Of interest to', 'Transkribus text available', 'Written by', 'Library and Signature', 'ids', 'id', 'date_min', 'date_max', 'filename', 'file_year']\n",
      "\n",
      "Unique values in 'Place':\n",
      "Place\n",
      "Paris                          69\n",
      "Amsterdam                      49\n",
      "Basel                          48\n",
      "Venice                         48\n",
      "London                         40\n",
      "Leipzig                        36\n",
      "Rome                           34\n",
      "Zurich                         33\n",
      "Leiden                         29\n",
      "Frankfurt am Main              26\n",
      "Göttingen                      25\n",
      "Tübingen                       25\n",
      "Nuremberg                      21\n",
      "Bologna                        21\n",
      "Strasbourg                     20\n",
      "Lyon                           19\n",
      "Wittenberg                     17\n",
      "Innsbruck                      16\n",
      "Cologne                        13\n",
      "Padua                          13\n",
      "Naples                         12\n",
      "Florence                       12\n",
      "Leiden, Stockholm, Erlangen    10\n",
      "Halle                          10\n",
      "Antwerp                        10\n",
      "Oxford                          8\n",
      "Copenhagen                      8\n",
      "Vienna                          8\n",
      "Bern                            7\n",
      "Augsburg                        7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of unique values in 'Place': 173\n",
      "Number of missing values in 'Place': 4\n",
      "\n",
      "Sample raw entries (up to first 20 non-null):\n",
      "['[Turku]', 'Liegnitz, Wrocław', 'Salamanca', 'Heidelberg', 'London', 'Oxford', 'Lund', 'Strasbourg', 'Basel', 'Basel', 'Basel', 'Basel', 'Basel', 'Bologna', 'Leipzig', 'Zurich', 'Venice', 'Rome', 'Herborn', 'Frankfurt am Main']\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Data Exploration\n",
    "# Display DataFrame Columns\n",
    "print(\"Columns in noscemus_metadata:\")\n",
    "print(noscemus_metadata.columns.tolist())\n",
    "\n",
    "# Replace 'candidate_column_name' with a column name from the list above\n",
    "candidate_column_name = 'Place' # <-- CHANGE THIS VALUE \n",
    "\n",
    "if candidate_column_name in noscemus_metadata.columns:\n",
    "    print(f\"\\nUnique values in '{candidate_column_name}':\")\n",
    "    # Display a sample of unique values and their counts\n",
    "    print(noscemus_metadata[candidate_column_name].value_counts().head(30))\n",
    "    print(f\"\\nNumber of unique values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].nunique()}\")\n",
    "    print(f\"Number of missing values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].isnull().sum()}\")\n",
    "    # Show some raw examples of the data in this column\n",
    "    print(\"\\nSample raw entries (up to first 20 non-null):\")\n",
    "    print(noscemus_metadata[candidate_column_name].dropna().head(20).tolist())\n",
    "else:\n",
    "    print(f\"Column '{candidate_column_name}' not found in DataFrame. Please choose from the list printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cline_extract_place_column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174 unique raw place mentions from 'Place'.\n",
      "Sample of raw places (first 50):\n",
      "['[Turku]' 'Liegnitz, Wrocław' 'Salamanca' 'Heidelberg' 'London' 'Oxford'\n",
      " 'Lund' 'Strasbourg' 'Basel' 'Bologna' 'Leipzig' 'Zurich' 'Venice' 'Rome'\n",
      " 'Herborn' 'Frankfurt am Main' 'Turin' 'Florence' 'Alcalá de Henares'\n",
      " 'Leiden' 'Innsbruck' 'London, Westminster Abbey' 'Paris' 'Cambridge'\n",
      " '[Landshut]' '[Ingolstadt]' 'Milan' 'Bergamo' 'Stuttgart' 'Perugia'\n",
      " 'Lyon' 's.l.' 'Amsterdam' '[Wittenberg]' 'Copenhagen' 'Padua' '[Padua]'\n",
      " 'Rimini' 'Büdingen' 'Königsberg' 'Uppsala' 'Stockholm, Uppsala, Turku'\n",
      " 'Leipzig, Desau' 'Würzburg' 'Saint Petersburg' 'Antwerp' 'Graz' 'Aachen'\n",
      " 'Göttingen' 'Târgu Mureș']\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Data Extraction - Extract 'Place' column\n",
    "actual_publication_place_column = 'Place'\n",
    "places_series = noscemus_metadata[actual_publication_place_column].astype(str).str.strip()\n",
    "unique_raw_places = places_series.unique()\n",
    "print(f\"Found {len(unique_raw_places)} unique raw place mentions from '{actual_publication_place_column}'.\")\n",
    "print(\"Sample of raw places (first 50):\")\n",
    "print(unique_raw_places[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cline_clean_places",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 138 unique cleaned place names after initial cleaning.\n",
      "Sample of cleaned places (first 50):\n",
      "['Aachen', 'Aberdeen', 'Aga', 'Alcal De Henares', 'Altdorf', 'Amsterdam', 'Antwerp', 'Arnhem', 'Augsburg', 'Baltimore', 'Basel', 'Bdingen', 'Bergamo', 'Berlin', 'Bern', 'Beromnster', 'Beuthen', 'Bologna', 'Bracciano', 'Bratislava', 'Breslau', 'Brussels', 'Caen', 'Cambridge', 'Cologne', 'Copenhagen', 'Danzig', 'Delft', 'Den Haag', 'Duisburg', 'Edinburgh', 'Erfurt', 'Ferrara', 'Florence', 'Franeker', 'Frankfurt Am Main', 'Frankfurt An Der Oder', 'Gdask', 'Gent', 'Gera', 'Ghent', 'Gouda', 'Graz', 'Grenoble', 'Groningen', 'Gstrow', 'Gttingen', 'Haarlem', 'Hagenau', 'Halle']\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Cleaning Publication Places\n",
    "\n",
    "def clean_place_name(name):\n",
    "    name_str = str(name).lower().strip() # Ensure string, lowercase, strip whitespace\n",
    "\n",
    "    # Remove content in brackets and parentheses\n",
    "    name_str = re.sub(r\"\\(.*?\\)\", \"\", name_str).strip()\n",
    "    name_str = re.sub(r\"\\[.*?\\]\", \"\", name_str).strip()\n",
    "\n",
    "    # Specific known substitutions (expand as needed)\n",
    "    replacements = {\n",
    "        \"lvgduni batavorvm\": \"leiden\",\n",
    "        \"lugduni batavorum\": \"leiden\",\n",
    "        \"amstelodami\": \"amsterdam\",\n",
    "        \"amstelædami\": \"amsterdam\",\n",
    "        \"amstelodamum\": \"amsterdam\",\n",
    "        \"parisiis\": \"paris\",\n",
    "        \"londini\": \"london\",\n",
    "        \"franequerae\": \"franeker\",\n",
    "        \"hafniae\": \"copenhagen\",\n",
    "        \"coloniae agrippinae\": \"cologne\",\n",
    "        \"antverpiae\": \"antwerp\",\n",
    "        \"lipsiae\": \"leipzig\",\n",
    "        \"argentorati\": \"strasbourg\",\n",
    "        \"s.l.\": \"\", # Sine loco (without place)\n",
    "        \"s. l.\": \"\",\n",
    "        \"s.a.\": \"\", # Sine anno (less relevant for place)\n",
    "        \"s.n.\": \"\",  # Sine nomine (less relevant for place)\n",
    "        \"o.o.\": \"\", # Ohne Ort (German for without place)\n",
    "        \"o. o.\": \"\"\n",
    "    }\n",
    "    name_str = replacements.get(name_str, name_str)\n",
    "\n",
    "    # For entries with multiple places like \"Liegnitz, Wrocław\", we'll try to take the first one for now.\n",
    "    # A more advanced approach could be to split and geocode each, but that adds complexity.\n",
    "    # Geonames might be able to handle some of these directly or find the primary city.\n",
    "    if ',' in name_str:\n",
    "        # Attempt to split by common delimiters like comma or semicolon\n",
    "        parts = re.split(r'[,;/&]', name_str)\n",
    "        # Take the first part, clean it further if necessary\n",
    "        name_str = parts[0].strip()\n",
    "        # Re-apply replacements if the first part is a known one\n",
    "        name_str = replacements.get(name_str, name_str)\n",
    "    \n",
    "    # Remove most remaining non-alphanumeric characters, except hyphens and apostrophes within words\n",
    "    name_str = re.sub(r\"[^a-z\\s'’-]\", \"\", name_str, flags=re.UNICODE).strip()\n",
    "    # Normalize multiple spaces to single space\n",
    "    name_str = re.sub(r\"\\s+\", \" \", name_str).strip()\n",
    "\n",
    "    return name_str.title() if name_str else \"\" # Title case for Geonames, or empty if cleaning resulted in nothing\n",
    "\n",
    "# Apply the cleaning function\n",
    "# Handle potential NaN values in 'places_series' by filling them with an empty string before applying\n",
    "cleaned_places_list = sorted(list(set(places_series.fillna('').apply(clean_place_name).tolist())))\n",
    "cleaned_places_list = [p for p in cleaned_places_list if p] # Remove empty strings that might result from cleaning\n",
    "\n",
    "print(f\"Found {len(cleaned_places_list)} unique cleaned place names after initial cleaning.\")\n",
    "print(\"Sample of cleaned places (first 50):\")\n",
    "print(cleaned_places_list[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81b6bc86286f85",
   "metadata": {},
   "source": [
    "All mapping between the metadata and the actual textual data happens through the \"id\" column.\n",
    "Thus, knowing its ID, you can load full textual data (both raw and morphologically annotated) any text or a subset of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5af301324b4cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:12.172150Z",
     "start_time": "2025-05-01T14:52:12.148337Z"
    }
   },
   "outputs": [],
   "source": [
    "id = 1378359\n",
    "base_url = \"https://ccs-lab.zcu.cz/noscemus_sents_data/{}.json\"\n",
    "sents_data = json.load(io.BytesIO(requests.get(base_url.format(str(id))).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba05d7351ab68f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:15.229801Z",
     "start_time": "2025-05-01T14:52:15.221690Z"
    }
   },
   "outputs": [],
   "source": [
    "# the sents_data is a list of sentences from the given document\n",
    "# in addition to the raw text of the sentence, it also contains the lemmatized tokens and their POS tags\n",
    "# look at first few sentences to get an idea of the format:\n",
    "sents_data[110:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc77f8e9f28243",
   "metadata": {},
   "source": [
    "For each sentence, you see the following elements:\n",
    "* (1) ID of the source document\n",
    "* (2) index of the sentence (remember that Python's indexing starts with 0)\n",
    "* (2) token data for the sentence\n",
    "\n",
    "The token data for each token contain:\n",
    "   * (a) The token as it is in the sentence\n",
    "   * (b) The automatically assigned lemma corresponding to the token\n",
    "   * (c) Its Part-of-Speech\n",
    "   * (d) Its starting positional index within the sentence\n",
    "   * (e) Its ending positional index within the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccdfe8-d52f-4cff-9393-41740d716807",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:22.579038Z",
     "start_time": "2025-05-01T14:52:22.574248Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you want a raw text of the document, use the following:\n",
    "rawtext = \" \".join([sent_data[2] for sent_data in sents_data])\n",
    "rawtext[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350a19f841d1a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:25.715683Z",
     "start_time": "2025-05-01T14:52:25.659753Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you want a list of lemmatized tokens, filtered by certain POS-tags, use the following:\n",
    "lemmatized_sents = []\n",
    "for sent_data in sents_data:\n",
    "    lemmatized_sent = []\n",
    "    for token in sent_data[3]:\n",
    "        if token[2] in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]:\n",
    "            lemmatized_sent.append(token[0])\n",
    "    lemmatized_sents.append(lemmatized_sent)\n",
    "lemmatized_sents[150:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5c6bfe44e5aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:52.824349Z",
     "start_time": "2025-05-01T14:52:38.544378Z"
    }
   },
   "outputs": [],
   "source": [
    "# based on the metadata, you can easily focus on a subset of documents\n",
    "# for instance, we want to focus on all texts from the first two decades of the 17th century:\n",
    "\n",
    "noscemus_subset = noscemus_metadata[noscemus_metadata[\"file_year\"].between(1600, 1620)]\n",
    "# to work with the subset, we need to know the IDs of the documents\n",
    "ids = noscemus_subset[\"id\"]\n",
    "# Subsequently, we can load the data for each document by its ID and calculate the vocabulary of the texts:\n",
    "# (depending on the size of the subset and your internet connection, this may take a while)\n",
    "base_url = \"https://ccs-lab.zcu.cz/noscemus_sents_data/{}.json\"\n",
    "subset_lemmatized_sentences = []\n",
    "for id in ids: # for each work ID from our subset of IDs\n",
    "    f_sents_data = json.load(io.BytesIO(requests.get(base_url.format(str(id))).content))\n",
    "    sents_n = len(f_sents_data)\n",
    "    for sent_data in f_sents_data:\n",
    "        sent_lemmata = [t[1] for t in sent_data[3] if t[2] in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]] # filter for specific POS-tags\n",
    "        sent_lemmata = [re.sub(r\"\\W*|\\d*\", \"\", t) for t in sent_lemmata] # remove all non-alphanumeric characters\n",
    "        sent_lemmata = [l for l in sent_lemmata if len(l) > 1] # remove all one-letter words\n",
    "        sent_lemmata = [l.lower() for l in sent_lemmata] # lowercase all words\n",
    "        subset_lemmatized_sentences.append(sent_lemmata) # add the lemmatized words from the current sentence to the overall list of lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ce8fea4b58e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:53:53.096771Z",
     "start_time": "2025-05-01T14:53:53.092207Z"
    }
   },
   "outputs": [],
   "source": [
    "# now you have lemmatized sentences for all texts in the subset\n",
    "# let's take a look at the first few sentences:'\n",
    "subset_lemmatized_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3dadbd38792e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:53:57.939839Z",
     "start_time": "2025-05-01T14:53:56.871715Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can flatten this list of lists into a single list of lemmatized words:\n",
    "subset_lemmata = [lemma for sent in subset_lemmatized_sentences for lemma in sent]\n",
    "# this data can be used to calculate the vocabulary of the texts:\n",
    "subset_vocab = nltk.FreqDist(subset_lemmata).most_common()\n",
    "subset_vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5866db3bbf32ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:54:01.129858Z",
     "start_time": "2025-05-01T14:54:01.127806Z"
    }
   },
   "outputs": [],
   "source": [
    "# with lemmatized sentences, you can also immediately proceed to various kinds of co-occurrence analysis or word-embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
