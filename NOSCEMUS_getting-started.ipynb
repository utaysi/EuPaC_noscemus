{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CCS-ZCU/EuPaC_shared/blob/master/NOSCEMUS_getting-started.ipynb)\n",
    "\n",
    "This Jupyter notebook has been prepared for the EuPaC Hackathon and provides an easy way to start working with the NOSCEMUS dataset — no need to clone the entire repository or download additional data. It is fully compatible with cloud platforms like Google Colaboratory (click the badge above) and runs without requiring any specialized library installations.\n",
    "\n",
    "As such, it is intended as a starting point for EuPaC participants, including those with minimal coding experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Setup - Import Libraries\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "noscemus_metadata = pd.read_csv(\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\")\n",
    "noscemus_metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: Data Exploration\n",
    "# Display DataFrame Columns\n",
    "print(\"Columns in noscemus_metadata:\")\n",
    "print(noscemus_metadata.columns.tolist())\n",
    "\n",
    "# Replace 'candidate_column_name' with a column name from the list above\n",
    "candidate_column_name = 'Place' # <-- CHANGE THIS VALUE \n",
    "\n",
    "if candidate_column_name in noscemus_metadata.columns:\n",
    "    print(f\"\\nUnique values in '{candidate_column_name}':\")\n",
    "    # Display a sample of unique values and their counts\n",
    "    print(noscemus_metadata[candidate_column_name].value_counts().head(30))\n",
    "    print(f\"\\nNumber of unique values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].nunique()}\")\n",
    "    print(f\"Number of missing values in '{candidate_column_name}': {noscemus_metadata[candidate_column_name].isnull().sum()}\")\n",
    "    # Show some raw examples of the data in this column\n",
    "    print(\"\\nSample raw entries (up to first 20 non-null):\")\n",
    "    print(noscemus_metadata[candidate_column_name].dropna().head(20).tolist())\n",
    "else:\n",
    "    print(f\"Column '{candidate_column_name}' not found in DataFrame. Please choose from the list printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Data Extraction - Extract 'Place' column\n",
    "actual_publication_place_column = 'Place'\n",
    "places_series = noscemus_metadata[actual_publication_place_column].astype(str).str.strip()\n",
    "unique_raw_places = places_series.unique()\n",
    "print(f\"Found {len(unique_raw_places)} unique raw place mentions from '{actual_publication_place_column}'.\")\n",
    "print(\"Sample of raw places (first 50):\")\n",
    "print(unique_raw_places[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Cleaning Publication Places\n",
    "\n",
    "def clean_place_name(name):\n",
    "    name_str = str(name).lower().strip() # Ensure string, lowercase, strip whitespace\n",
    "\n",
    "    # Remove content in brackets and parentheses\n",
    "    name_str = re.sub(r\"\\(.*?\\)\", \"\", name_str).strip()\n",
    "    name_str = re.sub(r\"\\[.*?\\]\", \"\", name_str).strip()\n",
    "\n",
    "    # Specific known substitutions (expand as needed)\n",
    "    replacements = {\n",
    "        \"lvgduni batavorvm\": \"leiden\",\n",
    "        \"lugduni batavorum\": \"leiden\",\n",
    "        \"amstelodami\": \"amsterdam\",\n",
    "        \"amstelædami\": \"amsterdam\",\n",
    "        \"amstelodamum\": \"amsterdam\",\n",
    "        \"parisiis\": \"paris\",\n",
    "        \"londini\": \"london\",\n",
    "        \"franequerae\": \"franeker\",\n",
    "        \"hafniae\": \"copenhagen\",\n",
    "        \"coloniae agrippinae\": \"cologne\",\n",
    "        \"antverpiae\": \"antwerp\",\n",
    "        \"lipsiae\": \"leipzig\",\n",
    "        \"argentorati\": \"strasbourg\",\n",
    "        \"s.l.\": \"\", # Sine loco (without place)\n",
    "        \"s. l.\": \"\",\n",
    "        \"s.a.\": \"\", # Sine anno (less relevant for place)\n",
    "        \"s.n.\": \"\",  # Sine nomine (less relevant for place)\n",
    "        \"o.o.\": \"\", # Ohne Ort (German for without place)\n",
    "        \"o. o.\": \"\"\n",
    "    }\n",
    "    name_str = replacements.get(name_str, name_str)\n",
    "\n",
    "    # For entries with multiple places like \"Liegnitz, Wrocław\", we'll try to take the first one for now.\n",
    "    # A more advanced approach could be to split and geocode each, but that adds complexity.\n",
    "    # Geonames might be able to handle some of these directly or find the primary city.\n",
    "    if ',' in name_str:\n",
    "        # Attempt to split by common delimiters like comma or semicolon\n",
    "        parts = re.split(r'[,;/&]', name_str)\n",
    "        # Take the first part, clean it further if necessary\n",
    "        name_str = parts[0].strip()\n",
    "        # Re-apply replacements if the first part is a known one\n",
    "        name_str = replacements.get(name_str, name_str)\n",
    "    \n",
    "    # Remove most remaining non-alphanumeric characters, except hyphens and apostrophes within words\n",
    "    name_str = re.sub(r\"[^a-z\\s'’-]\", \"\", name_str, flags=re.UNICODE).strip()\n",
    "    # Normalize multiple spaces to single space\n",
    "    name_str = re.sub(r\"\\s+\", \" \", name_str).strip()\n",
    "\n",
    "    return name_str.title() if name_str else \"\" # Title case for Geonames, or empty if cleaning resulted in nothing\n",
    "\n",
    "# Apply the cleaning function\n",
    "# Handle potential NaN values in 'places_series' by filling them with an empty string before applying\n",
    "cleaned_places_list = sorted(list(set(places_series.fillna('').apply(clean_place_name).tolist())))\n",
    "cleaned_places_list = [p for p in cleaned_places_list if p] # Remove empty strings that might result from cleaning\n",
    "\n",
    "print(f\"Found {len(cleaned_places_list)} unique cleaned place names after initial cleaning.\")\n",
    "print(\"Sample of cleaned places (first 50):\")\n",
    "print(cleaned_places_list[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "All mapping between the metadata and the actual textual data happens through the \"id\" column.\n",
    "Thus, knowing its ID, you can load full textual data (both raw and morphologically annotated) any text or a subset of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1378359\n",
    "base_url = \"https://ccs-lab.zcu.cz/noscemus_sents_data/{}.json\"\n",
    "sents_data = json.load(io.BytesIO(requests.get(base_url.format(str(id))).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sents_data is a list of sentences from the given document\n",
    "# in addition to the raw text of the sentence, it also contains the lemmatized tokens and their POS tags\n",
    "# look at first few sentences to get an idea of the format:\n",
    "sents_data[110:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "For each sentence, you see the following elements:\n",
    "* (1) ID of the source document\n",
    "* (2) index of the sentence (remember that Python's indexing starts with 0)\n",
    "* (2) token data for the sentence\n",
    "\n",
    "The token data for each token contain:\n",
    "   * (a) The token as it is in the sentence\n",
    "   * (b) The automatically assigned lemma corresponding to the token\n",
    "   * (c) Its Part-of-Speech\n",
    "   * (d) Its starting positional index within the sentence\n",
    "   * (e) Its ending positional index within the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a raw text of the document, use the following:\n",
    "rawtext = \" \".join([sent_data[2] for sent_data in sents_data])\n",
    "rawtext[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a list of lemmatized tokens, filtered by certain POS-tags, use the following:\n",
    "lemmatized_sents = []\n",
    "for sent_data in sents_data:\n",
    "    lemmatized_sent = []\n",
    "    for token in sent_data[3]:\n",
    "        if token[2] in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]:\n",
    "            lemmatized_sent.append(token[0])\n",
    "    lemmatized_sents.append(lemmatized_sent)\n",
    "lemmatized_sents[150:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the metadata, you can easily focus on a subset of documents\n",
    "# for instance, we want to focus on all texts from the first two decades of the 17th century:\n",
    "\n",
    "noscemus_subset = noscemus_metadata[noscemus_metadata[\"file_year\"].between(1600, 1620)]\n",
    "# to work with the subset, we need to know the IDs of the documents\n",
    "ids = noscemus_subset[\"id\"]\n",
    "# Subsequently, we can load the data for each document by its ID and calculate the vocabulary of the texts:\n",
    "# (depending on the size of the subset and your internet connection, this may take a while)\n",
    "base_url = \"https://ccs-lab.zcu.cz/noscemus_sents_data/{}.json\"\n",
    "subset_lemmatized_sentences = []\n",
    "for id in ids: # for each work ID from our subset of IDs\n",
    "    f_sents_data = json.load(io.BytesIO(requests.get(base_url.format(str(id))).content))\n",
    "    sents_n = len(f_sents_data)\n",
    "    for sent_data in f_sents_data:\n",
    "        sent_lemmata = [t[1] for t in sent_data[3] if t[2] in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]] # filter for specific POS-tags\n",
    "        sent_lemmata = [re.sub(r\"\\W*|\\d*\", \"\", t) for t in sent_lemmata] # remove all non-alphanumeric characters\n",
    "        sent_lemmata = [l for l in sent_lemmata if len(l) > 1] # remove all one-letter words\n",
    "        sent_lemmata = [l.lower() for l in sent_lemmata] # lowercase all words\n",
    "        subset_lemmatized_sentences.append(sent_lemmata) # add the lemmatized words from the current sentence to the overall list of lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you have lemmatized sentences for all texts in the subset\n",
    "# let's take a look at the first few sentences:'\n",
    "subset_lemmatized_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can flatten this list of lists into a single list of lemmatized words:\n",
    "subset_lemmata = [lemma for sent in subset_lemmatized_sentences for lemma in sent]\n",
    "# this data can be used to calculate the vocabulary of the texts:\n",
    "subset_vocab = nltk.FreqDist(subset_lemmata).most_common()\n",
    "subset_vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with lemmatized sentences, you can also immediately proceed to various kinds of co-occurrence analysis or word-embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
