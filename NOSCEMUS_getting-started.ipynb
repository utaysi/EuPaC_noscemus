{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b001cbcc97cb66",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CCS-ZCU/EuPaC_shared/blob/master/NOSCEMUS_getting-started.ipynb)\n",
    "\n",
    "This Jupyter notebook has been prepared for the EuPaC Hackathon and provides an easy way to start working with the NOSCEMUS dataset â€” no need to clone the entire repository or download additional data. It is fully compatible with cloud platforms like Google Colaboratory (click the badge above) and runs without requiring any specialized library installations.\n",
    "\n",
    "As such, it is intended as a starting point for EuPaC participants, including those with minimal coding experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b2bdaa3269606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:17:07.109340Z",
     "start_time": "2025-05-01T14:17:07.105530Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256c66aa1cbbdbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:08.472459Z",
     "start_time": "2025-05-01T14:52:07.967666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "noscemus_metadata = pd.read_csv(\"https://raw.githubusercontent.com/CCS-ZCU/noscemus_ETF/refs/heads/master/data/metadata_table_long.csv\")\n",
    "noscemus_metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81b6bc86286f85",
   "metadata": {},
   "source": [
    "All mapping between the metadata and the actual textual data happens through the \"id\" column.\n",
    "Thus, knowing its ID, you can load full textual data (both raw and morphologically annotated) any text or a subset of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5af301324b4cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:12.172150Z",
     "start_time": "2025-05-01T14:52:12.148337Z"
    }
   },
   "outputs": [],
   "source": [
    "id = 1378359\n",
    "base_url = \"https://ccs-lab.zcu.cz/noscemus_sents_data/{}.json\"\n",
    "sents_data = json.load(io.BytesIO(requests.get(base_url.format(str(id))).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba05d7351ab68f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:15.229801Z",
     "start_time": "2025-05-01T14:52:15.221690Z"
    }
   },
   "outputs": [],
   "source": [
    "# the sents_data is a list of sentences from the given document\n",
    "# in addition to the raw text of the sentence, it also contains the lemmatized tokens and their POS tags\n",
    "# look at first few sentences to get an idea of the format:\n",
    "sents_data[110:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc77f8e9f28243",
   "metadata": {},
   "source": [
    "For each sentence, you see the following elements:\n",
    "* (1) ID of the source document\n",
    "* (2) index of the sentence (remember that Python's indexing starts with 0)\n",
    "* (2) token data for the sentence\n",
    "\n",
    "The token data for each token contain:\n",
    "   * (a) The token as it is in the sentence\n",
    "   * (b) The automatically assigned lemma corresponding to the token\n",
    "   * (c) Its Part-of-Speech\n",
    "   * (d) Its starting positional index within the sentence\n",
    "   * (e) Its ending positional index within the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccdfe8-d52f-4cff-9393-41740d716807",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:22.579038Z",
     "start_time": "2025-05-01T14:52:22.574248Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you want a raw text of the document, use the following:\n",
    "rawtext = \" \".join([sent_data[2] for sent_data in sents_data])\n",
    "rawtext[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350a19f841d1a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:25.715683Z",
     "start_time": "2025-05-01T14:52:25.659753Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you want a list of lemmatized tokens, filtered by certain POS-tags, use the following:\n",
    "lemmatized_sents = []\n",
    "for sent_data in sents_data:\n",
    "    lemmatized_sent = []\n",
    "    for token in sent_data[3]:\n",
    "        if token[2] in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]:\n",
    "            lemmatized_sent.append(token[0])\n",
    "    lemmatized_sents.append(lemmatized_sent)\n",
    "lemmatized_sents[150:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5c6bfe44e5aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:52:52.824349Z",
     "start_time": "2025-05-01T14:52:38.544378Z"
    }
   },
   "outputs": [],
   "source": [
    "# based on the metadata, you can easily focus on a subset of documents\n",
    "# for instance, we want to focus on all texts from the first two decades of the 17th century:\n",
    "\n",
    "noscemus_subset = noscemus_metadata[noscemus_metadata[\"file_year\"].between(1600, 1620)]\n",
    "# to work with the subset, we need to know the IDs of the documents\n",
    "ids = noscemus_subset[\"id\"]\n",
    "# Subsequently, we can load the data for each document by its ID and calculate the vocabulary of the texts:\n",
    "# (depending on the size of the subset and your internet connection, this may take a while)\n",
    "base_url = \"https://ccs-lab.zcu.cz/noscemus_sents_data/{}.json\"\n",
    "subset_lemmatized_sentences = []\n",
    "for id in ids: # for each work ID from our subset of IDs\n",
    "    f_sents_data = json.load(io.BytesIO(requests.get(base_url.format(str(id))).content))\n",
    "    sents_n = len(f_sents_data)\n",
    "    for sent_data in f_sents_data:\n",
    "        sent_lemmata = [t[1] for t in sent_data[3] if t[2] in [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"]] # filter for specific POS-tags\n",
    "        sent_lemmata = [re.sub(r\"\\W*|\\d*\", \"\", t) for t in sent_lemmata] # remove all non-alphanumeric characters\n",
    "        sent_lemmata = [l for l in sent_lemmata if len(l) > 1] # remove all one-letter words\n",
    "        sent_lemmata = [l.lower() for l in sent_lemmata] # lowercase all words\n",
    "        subset_lemmatized_sentences.append(sent_lemmata) # add the lemmatized words from the current sentence to the overall list of lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ce8fea4b58e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:53:53.096771Z",
     "start_time": "2025-05-01T14:53:53.092207Z"
    }
   },
   "outputs": [],
   "source": [
    "# now you have lemmatized sentences for all texts in the subset\n",
    "# let's take a look at the first few sentences:'\n",
    "subset_lemmatized_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3dadbd38792e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:53:57.939839Z",
     "start_time": "2025-05-01T14:53:56.871715Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can flatten this list of lists into a single list of lemmatized words:\n",
    "subset_lemmata = [lemma for sent in subset_lemmatized_sentences for lemma in sent]\n",
    "# this data can be used to calculate the vocabulary of the texts:\n",
    "subset_vocab = nltk.FreqDist(subset_lemmata).most_common()\n",
    "subset_vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5866db3bbf32ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:54:01.129858Z",
     "start_time": "2025-05-01T14:54:01.127806Z"
    }
   },
   "outputs": [],
   "source": [
    "# with lemmatized sentences, you can also immediately proceed to various kinds of co-occurrence analysis or word-embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
